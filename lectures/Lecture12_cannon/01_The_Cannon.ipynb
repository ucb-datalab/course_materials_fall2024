{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T00:13:49.854175Z",
     "iopub.status.busy": "2024-10-14T00:13:49.852865Z",
     "iopub.status.idle": "2024-10-14T00:13:49.921847Z",
     "shell.execute_reply": "2024-10-14T00:13:49.918341Z",
     "shell.execute_reply.started": "2024-10-14T00:13:49.854016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\" integrity=\"sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "<style>\n",
       "\n",
       "@import url(https://fonts.googleapis.com/css?family=Open+Sans);body{\n",
       "   font-family: 'Open Sans';\n",
       "   font-size: 125%;\n",
       "}\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 275%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.3; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of \"The Cannon\"\n",
    "\n",
    "Astro 128/256 (UC Berkeley, 2024)\n",
    "\n",
    "From [Ness et al. (2015)](https://ui.adsabs.harvard.edu/abs/2015ApJ...808...16N/abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* A few of your stars have good labels from somewhere\n",
    "* How can you quickly and efficienctly transfer them to stars without labels?\n",
    "* Why do this?\n",
    "    * No good models at wavelengths of interest\n",
    "    * Two surveys on the same \"system\"\n",
    "    * Stars at a variety of SNRs\n",
    "    * Model-based fitting of each star prohibitively expensive (e.g., too many stars)\n",
    "* Two conceptual steps\n",
    "    * Training step -- use high quality data to train and validate your model\n",
    "    * Test step -- assume that the model created in the training step applies to all spectra in survey and apply the model\n",
    "    * Together these two steps result in \"label transfer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Select a Training Set (Sec 2.2)\n",
    "\n",
    "* The set of reference objects is critical, as the label transfer to the survey objects can only be as good as the quality of the reference label set.\n",
    "\n",
    "* 542 stars from 19 clusters \n",
    " * Clusters have an advantage over field stars: all stars have the same age, [Fe/H], distance, ...\n",
    "* Use known labels from APOGEE/ASPCAP pipeline ($T_{eff}$, log g, [$\\alpha$/Fe], [C/M], [N/M], micro-turbulence)\n",
    "* One goal is to place all stellar labels (true and inferred) on a common scale.\n",
    "\n",
    "__Figure 1 from Ness et al.:__ ASPCAP-corrected training set for The Cannon:\n",
    "<img src=\"figs/ness2015_fig1.jpg\" width=700 height=700></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Training Set\n",
    "\n",
    "* ASPCAP only includes stars with SNR>70 and log g < 3.5\n",
    "    * No dwarf stars\n",
    "* Pleiades is the only cluster with dwarf stars (small training set may make it hard to assign dwarf star labels)\n",
    "* For clusters, stellar parameters should lie on single isochrone\n",
    "* \"Isochrone-corrected labels\": modify all training stars so their logg values agrees with with single Padova isochrone based on literature values of cluster age and metallicity\n",
    "\n",
    "__Figure 2 from Ness et al.:__ Isochrone-corrected training set for The Cannon:\n",
    "<img src=\"figs/ness2015_fig2.jpg\" width=700 height=700></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Continuum Normalization (Sec 2.3, 5.3)\n",
    "* In theory, continuum is defined as pixels that are not affected by any absoprtion or emission lines\n",
    "* In practice, very hard to find \"pure\" continuum in almost any spectrum\n",
    "* Good practice would be to find pixels that are not affected by changing any features in the model\n",
    "\n",
    "### In The Cannon:\n",
    "* First, define \"pseudo-continuum\" by using a polynomial to fit upper 90% of spectra as determined by a running median over 50A windows.  Effectively smooths out the spectrum.\n",
    "    * Effective, but S/N dependent\n",
    "* Second, using the above as an initialization, run The Cannon on training set to find continuum pixels (Sec 5.3).\n",
    "    * That is, using The Cannon, they find pixels that are minimally affected by changing stellar labels.\n",
    "    * They find very small S/N dependence in continuum determination this way\n",
    "* Fit 2nd order [Chebyshev polynomials](http://mathworld.wolfram.com/ChebyshevPolynomialoftheFirstKind.html) to continuum pixels in hand for each of the 3 chips (15150-15800A, 15890-16430A, 16490-16950A)\n",
    "    * Polynomials poorly contrained at boundaries\n",
    " \n",
    "__Figure 3 from Ness et al.:__ Example Continuum Normalized Spectra:\n",
    "<img src=\"figs/ness2015_fig3.jpg\" width=700 height=700></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T00:13:49.926020Z",
     "iopub.status.busy": "2024-10-14T00:13:49.925066Z",
     "iopub.status.idle": "2024-10-14T00:13:50.216822Z",
     "shell.execute_reply": "2024-10-14T00:13:50.215860Z",
     "shell.execute_reply.started": "2024-10-14T00:13:49.925903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolynomial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChebyshev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdeg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Least squares fit to data.\n",
       "\n",
       "Return a series instance that is the least squares fit to the data\n",
       "`y` sampled at `x`. The domain of the returned instance can be\n",
       "specified and this will often result in a superior fit with less\n",
       "chance of ill conditioning.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "x : array_like, shape (M,)\n",
       "    x-coordinates of the M sample points ``(x[i], y[i])``.\n",
       "y : array_like, shape (M,)\n",
       "    y-coordinates of the M sample points ``(x[i], y[i])``.\n",
       "deg : int or 1-D array_like\n",
       "    Degree(s) of the fitting polynomials. If `deg` is a single integer\n",
       "    all terms up to and including the `deg`'th term are included in the\n",
       "    fit. For NumPy versions >= 1.11.0 a list of integers specifying the\n",
       "    degrees of the terms to include may be used instead.\n",
       "domain : {None, [beg, end], []}, optional\n",
       "    Domain to use for the returned series. If ``None``,\n",
       "    then a minimal domain that covers the points `x` is chosen.  If\n",
       "    ``[]`` the class domain is used. The default value was the\n",
       "    class domain in NumPy 1.4 and ``None`` in later versions.\n",
       "    The ``[]`` option was added in numpy 1.5.0.\n",
       "rcond : float, optional\n",
       "    Relative condition number of the fit. Singular values smaller\n",
       "    than this relative to the largest singular value will be\n",
       "    ignored. The default value is len(x)*eps, where eps is the\n",
       "    relative precision of the float type, about 2e-16 in most\n",
       "    cases.\n",
       "full : bool, optional\n",
       "    Switch determining nature of return value. When it is False\n",
       "    (the default) just the coefficients are returned, when True\n",
       "    diagnostic information from the singular value decomposition is\n",
       "    also returned.\n",
       "w : array_like, shape (M,), optional\n",
       "    Weights. If not None, the weight ``w[i]`` applies to the unsquared\n",
       "    residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are\n",
       "    chosen so that the errors of the products ``w[i]*y[i]`` all have\n",
       "    the same variance.  When using inverse-variance weighting, use\n",
       "    ``w[i] = 1/sigma(y[i])``.  The default value is None.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "window : {[beg, end]}, optional\n",
       "    Window to use for the returned series. The default\n",
       "    value is the default class domain\n",
       "\n",
       "    .. versionadded:: 1.6.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "new_series : series\n",
       "    A series that represents the least squares fit to the data and\n",
       "    has the domain and window specified in the call. If the\n",
       "    coefficients for the unscaled and unshifted basis polynomials are\n",
       "    of interest, do ``new_series.convert().coef``.\n",
       "\n",
       "[resid, rank, sv, rcond] : list\n",
       "    These values are only returned if ``full == True``\n",
       "\n",
       "    - resid -- sum of squared residuals of the least squares fit\n",
       "    - rank -- the numerical rank of the scaled Vandermonde matrix\n",
       "    - sv -- singular values of the scaled Vandermonde matrix\n",
       "    - rcond -- value of `rcond`.\n",
       "\n",
       "    For more details, see `linalg.lstsq`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/numpy/polynomial/_polybase.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.polynomial.Chebyshev.fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training the Generative Model (Sec 3)\n",
    "\n",
    "### Underlying assumptions:\n",
    "* Continuum-normalized spectra of stars with identical lablels look nearly-identical at every pixel\n",
    "    * Really an approximation since number of labels used is not exhaustive\n",
    "* Expected flux at every pixel changes continuously as a function of the labels\n",
    "* Model is generative: produces a PDF for the flux at every pixel/wavelength.\n",
    "\n",
    "### Define variable:\n",
    "* Number of reference stars: $N_{ref} = n$  \n",
    "    * Each has continuum-normalized flux measurement $f_{n \\lambda}$ at each wavelength $\\lambda$\n",
    "* Each of the training spectra (of index $n$) has $K$ labels $\\ell_{nk}$\n",
    "    * Assume labels are prefectly known.\n",
    "* label vector: $\\ell_n$\n",
    "* For any star, $n$ , at any pixel $\\lambda$, the flux $f_{n \\lambda}$ can be described as a smooth function of the star's labels: $\\ell(T_{eff}, \\log g, [Fe/H], \\cdots)$\n",
    "\n",
    "### Uncertainties\n",
    "* Observational uncertainties at each pixel: $\\sigma_{n \\lambda}$\n",
    "* Second noise term to account of possible systematics, other variations per pixel: $s_{n \\lambda}$\n",
    "    * Presumed to be Gaussian\n",
    "\n",
    "### Write down model:\n",
    "\n",
    "* Predict the flux $f_{n \\lambda}$ at every pixel, given label and coefficient vectors: $\\ell_n$ and $\\theta_\\lambda$\n",
    "* Likelihood: $\\rm{ln} \\ p (f_n\\ | \\ l_n, \\theta) = \\sum_{\\lambda=1}^{L} \\rm{ln} \\ p(f_{n \\lambda} \\ |\\ \\ell_n, \\theta_{\\lambda}, s^2_{\\lambda})$\n",
    "\n",
    "* Single pixel Likelihood: $\\rm{ln}\\ p(f_{n \\lambda} \\ |\\ l_n, \\theta_{\\lambda}, s^2_{\\lambda}) = - \\frac{1}{2} \\frac{[f_{n \\lambda} - \\theta_{\\lambda}^{T} \\dot\\ \\ell_{n}]^2}{\\sigma_{n \\lambda}^2 + s_\\lambda^2}$ + $\\rm{ln}\\ (\\sigma_{n \\lambda}^2 + s_\\lambda^2)$\n",
    "\n",
    "* $\\ell^T \\equiv \\{1, T_{eff}, \\log g, [Fe/H], T_{eff}^2, T_{eff} \\log g, \\cdots, [Fe/H]^2\\}$\n",
    "    * This vector contains permutations of stellar labels\n",
    "    * Simplest model would be linear model (Eqn 5 in Ness et al.)\n",
    "    * This is the quadratic model (Eqn 6 in Ness et al.)\n",
    "\n",
    "* $\\theta^T \\equiv \\{\\theta_{\\lambda}, s_{\\lambda}^2 \\}_{\\lambda=1}^{L}$\n",
    "    * This vector is a vector of coefficients\n",
    "    * Every pixel (i.e., every wavelength) has a set of coefficients\n",
    "\n",
    "* Some interpretation:\n",
    "    * $\\theta_{\\lambda 0}$: baseline spectrum\n",
    "    * $\\theta_{\\lambda k}$: first order coefficients are first derivatives of spectrum wrt to linear labels\n",
    "    * $\\theta_{\\lambda k k^\\prime}$: second order coefficients are second derivatives of spectrum wrt to quadratic labels\n",
    " \n",
    "### Training the model\n",
    "\n",
    "* For training set $f_{n \\lambda}$ and $\\ell_n$ are known\n",
    "* Thus, can solve for $\\theta_{\\lambda}$ and $s_{\\lambda}$\n",
    "* Could use MCMC, but would be very slow ($\\sim$80,000 values of $\\theta_{\\lambda}$)\n",
    "* Use non-linear optimization (non-linear becuase of quadratic labels and noise $s_\\lambda^2$)\n",
    "* $\\theta_{\\lambda}, s_{\\lambda} \\leftarrow \\rm{argmax}(\\theta_{\\lambda}, s_{\\lambda}) \\sum_{n=1}^{N} \\ \\rm{ln} \\ p(f_{n, \\lambda} \\ | \\ \\theta_{\\lambda}, \\ell_{\\lambda}, s_{\\lambda}^2)$\n",
    "* Consider all pixels in survey, one pixel at a time\n",
    "\n",
    "\n",
    "### A very simple conceptual example\n",
    "\n",
    "* Suppose we only had two labels: $T_{\\rm eff}$ and [Fe/H] \n",
    "* A quadratic model for the flux at one pixel would be:\n",
    "    - $f$ = $\\theta_0$ + $\\theta_1$ $T_{\\rm eff}$ + $\\theta_2$ [Fe/H] + $\\theta_3$ $T_{\\rm eff}$[Fe/H] + $\\theta_4$ $T_{\\rm eff}^2$ + $\\theta_5$ [Fe/H]$^2$\n",
    "    - repeat procedure summing over all pixels\n",
    "    - use the training data to find values for $\\theta_k$ (i.e., you have labels, solve for $\\theta_k$)\n",
    "    - for validation/real data: you know each $\\theta_k$ and you want to assign a lablel for each star\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example non-linear optimization routines in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T00:13:50.217806Z",
     "iopub.status.busy": "2024-10-14T00:13:50.217510Z",
     "iopub.status.idle": "2024-10-14T00:13:50.376574Z",
     "shell.execute_reply": "2024-10-14T00:13:50.375620Z",
     "shell.execute_reply.started": "2024-10-14T00:13:50.217783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mcurve_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mxdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mydata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mp0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mabsolute_sigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfull_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Use non-linear least squares to fit a function, f, to data.\n",
       "\n",
       "Assumes ``ydata = f(xdata, *params) + eps``.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "f : callable\n",
       "    The model function, f(x, ...). It must take the independent\n",
       "    variable as the first argument and the parameters to fit as\n",
       "    separate remaining arguments.\n",
       "xdata : array_like\n",
       "    The independent variable where the data is measured.\n",
       "    Should usually be an M-length sequence or an (k,M)-shaped array for\n",
       "    functions with k predictors, and each element should be float\n",
       "    convertible if it is an array like object.\n",
       "ydata : array_like\n",
       "    The dependent data, a length M array - nominally ``f(xdata, ...)``.\n",
       "p0 : array_like, optional\n",
       "    Initial guess for the parameters (length N). If None, then the\n",
       "    initial values will all be 1 (if the number of parameters for the\n",
       "    function can be determined using introspection, otherwise a\n",
       "    ValueError is raised).\n",
       "sigma : None or M-length sequence or MxM array, optional\n",
       "    Determines the uncertainty in `ydata`. If we define residuals as\n",
       "    ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n",
       "    depends on its number of dimensions:\n",
       "\n",
       "        - A 1-D `sigma` should contain values of standard deviations of\n",
       "          errors in `ydata`. In this case, the optimized function is\n",
       "          ``chisq = sum((r / sigma) ** 2)``.\n",
       "\n",
       "        - A 2-D `sigma` should contain the covariance matrix of\n",
       "          errors in `ydata`. In this case, the optimized function is\n",
       "          ``chisq = r.T @ inv(sigma) @ r``.\n",
       "\n",
       "          .. versionadded:: 0.19\n",
       "\n",
       "    None (default) is equivalent of 1-D `sigma` filled with ones.\n",
       "absolute_sigma : bool, optional\n",
       "    If True, `sigma` is used in an absolute sense and the estimated parameter\n",
       "    covariance `pcov` reflects these absolute values.\n",
       "\n",
       "    If False (default), only the relative magnitudes of the `sigma` values matter.\n",
       "    The returned parameter covariance matrix `pcov` is based on scaling\n",
       "    `sigma` by a constant factor. This constant is set by demanding that the\n",
       "    reduced `chisq` for the optimal parameters `popt` when using the\n",
       "    *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n",
       "    match the sample variance of the residuals after the fit. Default is False.\n",
       "    Mathematically,\n",
       "    ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n",
       "check_finite : bool, optional\n",
       "    If True, check that the input arrays do not contain nans of infs,\n",
       "    and raise a ValueError if they do. Setting this parameter to\n",
       "    False may silently produce nonsensical results if the input arrays\n",
       "    do contain nans. Default is True.\n",
       "bounds : 2-tuple of array_like or `Bounds`, optional\n",
       "    Lower and upper bounds on parameters. Defaults to no bounds.\n",
       "    There are two ways to specify the bounds:\n",
       "\n",
       "        - Instance of `Bounds` class.\n",
       "\n",
       "        - 2-tuple of array_like: Each element of the tuple must be either\n",
       "          an array with the length equal to the number of parameters, or a\n",
       "          scalar (in which case the bound is taken to be the same for all\n",
       "          parameters). Use ``np.inf`` with an appropriate sign to disable\n",
       "          bounds on all or some parameters.\n",
       "\n",
       "method : {'lm', 'trf', 'dogbox'}, optional\n",
       "    Method to use for optimization. See `least_squares` for more details.\n",
       "    Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n",
       "    provided. The method 'lm' won't work when the number of observations\n",
       "    is less than the number of variables, use 'trf' or 'dogbox' in this\n",
       "    case.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "jac : callable, string or None, optional\n",
       "    Function with signature ``jac(x, ...)`` which computes the Jacobian\n",
       "    matrix of the model function with respect to parameters as a dense\n",
       "    array_like structure. It will be scaled according to provided `sigma`.\n",
       "    If None (default), the Jacobian will be estimated numerically.\n",
       "    String keywords for 'trf' and 'dogbox' methods can be used to select\n",
       "    a finite difference scheme, see `least_squares`.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "full_output : boolean, optional\n",
       "    If True, this function returns additioal information: `infodict`,\n",
       "    `mesg`, and `ier`.\n",
       "\n",
       "    .. versionadded:: 1.9\n",
       "**kwargs\n",
       "    Keyword arguments passed to `leastsq` for ``method='lm'`` or\n",
       "    `least_squares` otherwise.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "popt : array\n",
       "    Optimal values for the parameters so that the sum of the squared\n",
       "    residuals of ``f(xdata, *popt) - ydata`` is minimized.\n",
       "pcov : 2-D array\n",
       "    The estimated covariance of popt. The diagonals provide the variance\n",
       "    of the parameter estimate. To compute one standard deviation errors\n",
       "    on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\n",
       "\n",
       "    How the `sigma` parameter affects the estimated covariance\n",
       "    depends on `absolute_sigma` argument, as described above.\n",
       "\n",
       "    If the Jacobian matrix at the solution doesn't have a full rank, then\n",
       "    'lm' method returns a matrix filled with ``np.inf``, on the other hand\n",
       "    'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n",
       "    the covariance matrix.\n",
       "infodict : dict (returned only if `full_output` is True)\n",
       "    a dictionary of optional outputs with the keys:\n",
       "\n",
       "    ``nfev``\n",
       "        The number of function calls. Methods 'trf' and 'dogbox' do not\n",
       "        count function calls for numerical Jacobian approximation,\n",
       "        as opposed to 'lm' method.\n",
       "    ``fvec``\n",
       "        The function values evaluated at the solution.\n",
       "    ``fjac``\n",
       "        A permutation of the R matrix of a QR\n",
       "        factorization of the final approximate\n",
       "        Jacobian matrix, stored column wise.\n",
       "        Together with ipvt, the covariance of the\n",
       "        estimate can be approximated.\n",
       "        Method 'lm' only provides this information.\n",
       "    ``ipvt``\n",
       "        An integer array of length N which defines\n",
       "        a permutation matrix, p, such that\n",
       "        fjac*p = q*r, where r is upper triangular\n",
       "        with diagonal elements of nonincreasing\n",
       "        magnitude. Column j of p is column ipvt(j)\n",
       "        of the identity matrix.\n",
       "        Method 'lm' only provides this information.\n",
       "    ``qtf``\n",
       "        The vector (transpose(q) * fvec).\n",
       "        Method 'lm' only provides this information.\n",
       "\n",
       "    .. versionadded:: 1.9\n",
       "mesg : str (returned only if `full_output` is True)\n",
       "    A string message giving information about the solution.\n",
       "\n",
       "    .. versionadded:: 1.9\n",
       "ier : int (returnned only if `full_output` is True)\n",
       "    An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n",
       "    found. Otherwise, the solution was not found. In either case, the\n",
       "    optional output variable `mesg` gives more information.\n",
       "\n",
       "    .. versionadded:: 1.9\n",
       "\n",
       "Raises\n",
       "------\n",
       "ValueError\n",
       "    if either `ydata` or `xdata` contain NaNs, or if incompatible options\n",
       "    are used.\n",
       "\n",
       "RuntimeError\n",
       "    if the least-squares minimization fails.\n",
       "\n",
       "OptimizeWarning\n",
       "    if covariance of the parameters can not be estimated.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "least_squares : Minimize the sum of squares of nonlinear functions.\n",
       "scipy.stats.linregress : Calculate a linear least squares regression for\n",
       "                         two sets of measurements.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Users should ensure that inputs `xdata`, `ydata`, and the output of `f`\n",
       "are ``float64``, or else the optimization may return incorrect results.\n",
       "\n",
       "With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n",
       "through `leastsq`. Note that this algorithm can only deal with\n",
       "unconstrained problems.\n",
       "\n",
       "Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n",
       "the docstring of `least_squares` for more information.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> from scipy.optimize import curve_fit\n",
       "\n",
       ">>> def func(x, a, b, c):\n",
       "...     return a * np.exp(-b * x) + c\n",
       "\n",
       "Define the data to be fit with some noise:\n",
       "\n",
       ">>> xdata = np.linspace(0, 4, 50)\n",
       ">>> y = func(xdata, 2.5, 1.3, 0.5)\n",
       ">>> rng = np.random.default_rng()\n",
       ">>> y_noise = 0.2 * rng.normal(size=xdata.size)\n",
       ">>> ydata = y + y_noise\n",
       ">>> plt.plot(xdata, ydata, 'b-', label='data')\n",
       "\n",
       "Fit for the parameters a, b, c of the function `func`:\n",
       "\n",
       ">>> popt, pcov = curve_fit(func, xdata, ydata)\n",
       ">>> popt\n",
       "array([2.56274217, 1.37268521, 0.47427475])\n",
       ">>> plt.plot(xdata, func(xdata, *popt), 'r-',\n",
       "...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
       "\n",
       "Constrain the optimization to the region of ``0 <= a <= 3``,\n",
       "``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n",
       "\n",
       ">>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n",
       ">>> popt\n",
       "array([2.43736712, 1.        , 0.34463856])\n",
       ">>> plt.plot(xdata, func(xdata, *popt), 'g--',\n",
       "...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
       "\n",
       ">>> plt.xlabel('x')\n",
       ">>> plt.ylabel('y')\n",
       ">>> plt.legend()\n",
       ">>> plt.show()\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/scipy/optimize/_minpack_py.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "curve_fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T00:13:50.377548Z",
     "iopub.status.busy": "2024-10-14T00:13:50.377278Z",
     "iopub.status.idle": "2024-10-14T00:13:50.382856Z",
     "shell.execute_reply": "2024-10-14T00:13:50.381948Z",
     "shell.execute_reply.started": "2024-10-14T00:13:50.377523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Least-squares fit of a polynomial to data.\n",
       "\n",
       "Return the coefficients of a polynomial of degree `deg` that is the\n",
       "least squares fit to the data values `y` given at points `x`. If `y` is\n",
       "1-D the returned coefficients will also be 1-D. If `y` is 2-D multiple\n",
       "fits are done, one for each column of `y`, and the resulting\n",
       "coefficients are stored in the corresponding columns of a 2-D return.\n",
       "The fitted polynomial(s) are in the form\n",
       "\n",
       ".. math::  p(x) = c_0 + c_1 * x + ... + c_n * x^n,\n",
       "\n",
       "where `n` is `deg`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "x : array_like, shape (`M`,)\n",
       "    x-coordinates of the `M` sample (data) points ``(x[i], y[i])``.\n",
       "y : array_like, shape (`M`,) or (`M`, `K`)\n",
       "    y-coordinates of the sample points.  Several sets of sample points\n",
       "    sharing the same x-coordinates can be (independently) fit with one\n",
       "    call to `polyfit` by passing in for `y` a 2-D array that contains\n",
       "    one data set per column.\n",
       "deg : int or 1-D array_like\n",
       "    Degree(s) of the fitting polynomials. If `deg` is a single integer\n",
       "    all terms up to and including the `deg`'th term are included in the\n",
       "    fit. For NumPy versions >= 1.11.0 a list of integers specifying the\n",
       "    degrees of the terms to include may be used instead.\n",
       "rcond : float, optional\n",
       "    Relative condition number of the fit.  Singular values smaller\n",
       "    than `rcond`, relative to the largest singular value, will be\n",
       "    ignored.  The default value is ``len(x)*eps``, where `eps` is the\n",
       "    relative precision of the platform's float type, about 2e-16 in\n",
       "    most cases.\n",
       "full : bool, optional\n",
       "    Switch determining the nature of the return value.  When ``False``\n",
       "    (the default) just the coefficients are returned; when ``True``,\n",
       "    diagnostic information from the singular value decomposition (used\n",
       "    to solve the fit's matrix equation) is also returned.\n",
       "w : array_like, shape (`M`,), optional\n",
       "    Weights. If not None, the weight ``w[i]`` applies to the unsquared\n",
       "    residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are\n",
       "    chosen so that the errors of the products ``w[i]*y[i]`` all have the\n",
       "    same variance.  When using inverse-variance weighting, use\n",
       "    ``w[i] = 1/sigma(y[i])``.  The default value is None.\n",
       "\n",
       "    .. versionadded:: 1.5.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "coef : ndarray, shape (`deg` + 1,) or (`deg` + 1, `K`)\n",
       "    Polynomial coefficients ordered from low to high.  If `y` was 2-D,\n",
       "    the coefficients in column `k` of `coef` represent the polynomial\n",
       "    fit to the data in `y`'s `k`-th column.\n",
       "\n",
       "[residuals, rank, singular_values, rcond] : list\n",
       "    These values are only returned if ``full == True``\n",
       "\n",
       "    - residuals -- sum of squared residuals of the least squares fit\n",
       "    - rank -- the numerical rank of the scaled Vandermonde matrix\n",
       "    - singular_values -- singular values of the scaled Vandermonde matrix\n",
       "    - rcond -- value of `rcond`.\n",
       "\n",
       "    For more details, see `numpy.linalg.lstsq`.\n",
       "\n",
       "Raises\n",
       "------\n",
       "RankWarning\n",
       "    Raised if the matrix in the least-squares fit is rank deficient.\n",
       "    The warning is only raised if ``full == False``.  The warnings can\n",
       "    be turned off by:\n",
       "\n",
       "    >>> import warnings\n",
       "    >>> warnings.simplefilter('ignore', np.RankWarning)\n",
       "\n",
       "See Also\n",
       "--------\n",
       "numpy.polynomial.chebyshev.chebfit\n",
       "numpy.polynomial.legendre.legfit\n",
       "numpy.polynomial.laguerre.lagfit\n",
       "numpy.polynomial.hermite.hermfit\n",
       "numpy.polynomial.hermite_e.hermefit\n",
       "polyval : Evaluates a polynomial.\n",
       "polyvander : Vandermonde matrix for powers.\n",
       "numpy.linalg.lstsq : Computes a least-squares fit from the matrix.\n",
       "scipy.interpolate.UnivariateSpline : Computes spline fits.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The solution is the coefficients of the polynomial `p` that minimizes\n",
       "the sum of the weighted squared errors\n",
       "\n",
       ".. math:: E = \\sum_j w_j^2 * |y_j - p(x_j)|^2,\n",
       "\n",
       "where the :math:`w_j` are the weights. This problem is solved by\n",
       "setting up the (typically) over-determined matrix equation:\n",
       "\n",
       ".. math:: V(x) * c = w * y,\n",
       "\n",
       "where `V` is the weighted pseudo Vandermonde matrix of `x`, `c` are the\n",
       "coefficients to be solved for, `w` are the weights, and `y` are the\n",
       "observed values.  This equation is then solved using the singular value\n",
       "decomposition of `V`.\n",
       "\n",
       "If some of the singular values of `V` are so small that they are\n",
       "neglected (and `full` == ``False``), a `RankWarning` will be raised.\n",
       "This means that the coefficient values may be poorly determined.\n",
       "Fitting to a lower order polynomial will usually get rid of the warning\n",
       "(but may not be what you want, of course; if you have independent\n",
       "reason(s) for choosing the degree which isn't working, you may have to:\n",
       "a) reconsider those reasons, and/or b) reconsider the quality of your\n",
       "data).  The `rcond` parameter can also be set to a value smaller than\n",
       "its default, but the resulting fit may be spurious and have large\n",
       "contributions from roundoff error.\n",
       "\n",
       "Polynomial fits using double precision tend to \"fail\" at about\n",
       "(polynomial) degree 20. Fits using Chebyshev or Legendre series are\n",
       "generally better conditioned, but much can still depend on the\n",
       "distribution of the sample points and the smoothness of the data.  If\n",
       "the quality of the fit is inadequate, splines may be a good\n",
       "alternative.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> np.random.seed(123)\n",
       ">>> from numpy.polynomial import polynomial as P\n",
       ">>> x = np.linspace(-1,1,51) # x \"data\": [-1, -0.96, ..., 0.96, 1]\n",
       ">>> y = x**3 - x + np.random.randn(len(x)) # x^3 - x + N(0,1) \"noise\"\n",
       ">>> c, stats = P.polyfit(x,y,3,full=True)\n",
       ">>> np.random.seed(123)\n",
       ">>> c # c[0], c[2] should be approx. 0, c[1] approx. -1, c[3] approx. 1\n",
       "array([ 0.01909725, -1.30598256, -0.00577963,  1.02644286]) # may vary\n",
       ">>> stats # note the large SSR, explaining the rather poor results\n",
       " [array([ 38.06116253]), 4, array([ 1.38446749,  1.32119158,  0.50443316, # may vary\n",
       "          0.28853036]), 1.1324274851176597e-014]\n",
       "\n",
       "Same thing without the added noise\n",
       "\n",
       ">>> y = x**3 - x\n",
       ">>> c, stats = P.polyfit(x,y,3,full=True)\n",
       ">>> c # c[0], c[2] should be \"very close to 0\", c[1] ~= -1, c[3] ~= 1\n",
       "array([-6.36925336e-18, -1.00000000e+00, -4.08053781e-16,  1.00000000e+00])\n",
       ">>> stats # note the minuscule SSR\n",
       "[array([  7.46346754e-31]), 4, array([ 1.38446749,  1.32119158, # may vary\n",
       "           0.50443316,  0.28853036]), 1.1324274851176597e-014]\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/numpy/polynomial/polynomial.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.polynomial import polynomial as P\n",
    "P.polyfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Labeling Survey Spectra (Sec 4)\n",
    "\n",
    "* \"Test data\": $M=m$ unlabeled spectra\n",
    "    * continuum-normalized flux $f_{m \\lambda}$ at each wavelength $\\lambda$\n",
    "    * associated uncertainty: $\\sigma_{m \\lambda}$\n",
    "\n",
    "* Assume spectral model coefficients from Step 3 and solve for labels of test data\n",
    "\n",
    "* $\\{\\ell_{m k}\\} \\leftarrow (\\rm{argmax})\\{\\ell_{m k} \\} \\sum_{\\lambda=1}^{N_{pix}} \\ \\rm{ln} \\ p(f_{m, \\lambda} \\ | \\ \\theta_{\\lambda}, \\ell_{m}, s_{\\lambda}^2)$\n",
    "\n",
    "* Follow same optimization procedure as above, only solving different variables\n",
    "* Optimize over all spectral model coefficients and scatter, considering all reference objects one pixel at a time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results and Validation\n",
    "\n",
    "### Model Selection (Sec 5.1)\n",
    "\n",
    "* What about only using a linear-in-labels model?  Turns out to be too inflexible.\n",
    "    * Show large and systematic deviations when run on objects with known labels\n",
    "    * Next simplest is the quadratic model.\n",
    "    * Is a quadratic model good enough? Beyond scope of paper.\n",
    " \n",
    "### Take-one-out Validation (Sec 5.2)\n",
    "\n",
    "* Use N-1 reference stars to train model\n",
    "* Run model on Nth star to predict its labels\n",
    "* Repeat $N_{ref}$ times\n",
    "* Only consider 3 labels ($T_{eff}, \\log g, [Fe/H]$)\n",
    "\n",
    "__Figure 4 from Ness et al.:__ Take-one-out Validation:\n",
    "<img src=\"figs/ness2015_fig4.jpg\" width=700 height=700></img>\n",
    "\n",
    "* The Cannon generally has smaller scatter than ASPCAP\n",
    "* They proceed to discuss trends and outliers in above plot.\n",
    "\n",
    "### Asessment of model performace (Sec 5.2)\n",
    "\n",
    "* Examine coefficients and scatter for example spectral regions A & B\n",
    "\n",
    "__Figure 5 from Ness et al.:__ Detailed look at one star:\n",
    "<img src=\"figs/ness2015_fig5.jpg\" width=700 height=700></img>\n",
    "\n",
    "\"Figure 5. First-order coefficients and scatter across the sample regions of the spectra from Figure 3, (A) and (B). Top panel: the baseline spectra representing the first coefficient from the set of reference spectra; middle panel: the next three coefficients (${\\theta }_{1}$, ${\\theta }_{2}$, ${\\theta }_{3}$), which correspond to the labels (${T}_{\\mathrm{eff}},\\mathrm{log}\\;g,[\\mathrm{Fe}/{\\rm{H}}]$ ); bottom panel: the scatter of the fit with a tenfold expanded vertical scale. The red, blue, and green areas in the top panel encompass the wavelength regions with the 5% highest (absolute value) coefficients for the $[\\mathrm{Fe}/{\\rm{H}}]$, $\\mathrm{log}\\;g$ and ${T}_{\\mathrm{eff}}$ labels, respectively. The ${T}_{\\mathrm{eff}}$ coefficient has been multiplied by a factor of 1000 simply to show this coefficient on a similar scale to the other coefficients. This indicates where the flux in these spectrum is particularly sensitive to the labels. Note that the $[\\mathrm{Fe}/{\\rm{H}}]$ label is dominant in the contribution level and from the top panel it is clear that there is significant covariance between the labels and there are only a few regions of $\\mathrm{log}\\;g$ sensitivity. The filled dots in the baseline spectrum in the top penal indicate the wavelengths at which the dependencies on all labels are weak, which we operatively identify as continuum pixels.\"\n",
    "\n",
    "* There are very few regions where the flux is a function of only one of the labels, and pixels are typically co-variant. (that is, the same pixel will have a higher flux at both lower ${T}_{\\mathrm{eff}}$ and higher $[\\mathrm{Fe}/{\\rm{H}}]$). This simply reflects well-known co-variances between, for example, temperature and $[\\mathrm{Fe}/{\\rm{H}}]$. The strongest $\\mathrm{log}\\;g$ dependence is typically associated with weak lines including the wings of the feature and the $[\\mathrm{Fe}/{\\rm{H}}]$ label, with strong lines, particularly the depth of the line.\n",
    "\n",
    "* The scatter is small and this indicates that our model is a good representation of the data. However, the scatter is highest where the most information in the spectra are contained. This implies that either our quadratic-in-labels spectral model is still somewhat too restricted, or that the labels of our training data set are imperfect or incomplete (for example, lacking $[\\alpha /\\mathrm{Fe}]$ as a label), or a combination of these effects.\n",
    "\n",
    "\n",
    "### Model-Data comparison (Sec 5.2)\n",
    "\n",
    "__Figure 6 from Ness et al.:__ Models + Scatter (cyan), data (black) for 4 stars:\n",
    "<img src=\"figs/ness2015_fig6.jpg\" width=1500 height=1500></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Application to APOGEE (Sec 5.4)\n",
    "\n",
    "__Figure 7 from Ness et al.:__ Application to APOGEE:\n",
    "<img src=\"figs/ness2015_fig7.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 7. ASPCAP DR10 vs. The Cannon for six different fields including in the disk, bulge, and halo. The number of stars for each subfigure is 211 (4431), 207 (4384), 217 (4399), 210 (4309), 198 (4311), 319 (4255). Each panel lists the mean difference between the labels (bias), the scatter between the labels (rms), and the formal uncertainly returned by The Cannon (precision).\n",
    "\n",
    "* There are weak trends; at low ${T}_{\\mathrm{eff}}$ ~ 3700 K, we find temperatures about 100 K cooler than APOGEE and at low $\\mathrm{log}\\;g$ we find ~0.15 dex larger $\\mathrm{log}\\;g$ than APOGEE. At the lowest metallicities $[\\mathrm{Fe}/{\\rm{H}}]\\;\\lt \\;-2.0$, we typically report higher metallicities on the order of 0.05 to 0.3 dex\n",
    "\n",
    "__Figure 8 from Ness et al.:__ Assessment of Biases:\n",
    "<img src=\"figs/ness2015_fig8.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 8. Difference between the labels (${T}_{\\mathrm{eff}}$, $\\mathrm{log}\\;g$, and $[\\mathrm{Fe}/{\\rm{H}}]$) derived by The Cannon and their ASPCAP DR10 values for all the 1400 stars shown in Figure 7. The error bars are dominated by those quoted by ASPCAP. There are systematic offsets at the coolest temperatures.\n",
    "\n",
    "### Comparison to isochrones\n",
    "\n",
    "__Figure 9 from Ness et al.:__ Comparison to expectations from isochrones:\n",
    "<img src=\"figs/ness2015_fig9.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 9. Labels for the ~35,000 stars from DR10 derived by The Cannon based on ASPCAP-corrected labels for the set of reference objects. The set of panels on the left shows ${T}_{\\mathrm{eff}}$–$\\mathrm{log}\\;g$ in four metallicity bins. There are ~19,000, 13,000, 1600, and 1000 stars in the most metal-rich to metal-poor metallicity bins, respectively. The isochrones plotted are 10 Gyr Padova isochrones at the metallicities marked in the upper left hand corners of each sub-panel. The panel on the right shows all stars colored in $[\\mathrm{Fe}/{\\rm{H}}]$ on the four isochrones. Note that the $\\mathrm{log}\\;g$ distribution at low $\\mathrm{log}\\;g$ is narrow and offset from the giant branch. Reference objects are shown as open circles.\n",
    "\n",
    "### Scatter in the labels\n",
    "\n",
    "* Rerun model with 20 bootstrap realizations of training set to quantify formal scatter in labels\n",
    "* Scatter largest in regions outside training set\n",
    "\n",
    "\n",
    "__Figure 11 from Ness et al.:__ Formal Scatter in Labels:\n",
    "<img src=\"figs/ness2015_fig11.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 11. Standard deviation in the labels returned in ${T}_{\\mathrm{eff}}$, $\\mathrm{log}\\;g$ and $[\\mathrm{Fe}/{\\rm{H}}]$, shown in the ${T}_{\\mathrm{eff}}$−$\\mathrm{log}\\;g$ plane, normalized by the optimization error on each measurement, for 20 bootstrapping tests of the training set. The representative sample of ~670 stars shown here has been drawn from an equal sampling of a grid spaced by 100 K in ${T}_{\\mathrm{eff}}$, 0.25 dex in $\\mathrm{log}\\;g$ and 0.25 dex in $[\\mathrm{Fe}/{\\rm{H}}]$ from the labels returned using the model trained on the isochrone-corrected reference objects. The location of the reference objects is shown in the gray shaded regions in the panel. Note the narrow region of reference objects also on the main sequence. The highest scatter in the labels is seen for regions where the labels are extrapolated. These figures are shown for the isochrone-corrected labels discussed in Section 2.4.\n",
    "\n",
    "\n",
    "\n",
    "### How well does extrapolation work?\n",
    "\n",
    "__Figure 12 from Ness et al.:__ Formal Scatter in Labels:\n",
    "<img src=\"figs/ness2015_fig12.jpg\" width=700 height=700></img>\n",
    "\n",
    "\n",
    "Figure 12. Difference in labels between The Cannon and ASPCAP indicating the regions of extrapolation where the difference in the labels extends beyond the estimated errors of the methods, due to the limited sampling of the reference objects which does not fully cover the label-space of the survey. The ASPCAP-corrected training labels were used to generate the model applied at the test step on the DR10 data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
