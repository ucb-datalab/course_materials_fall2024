{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ucb-datalab/course_materials_fall2024/blob/main/lectures/Lecture20_lab3_cnn/03_CNNs_pytorch.ipynb\" target=\"_blank\" rel=\"nofollow\">\n",
    "    <img src=\"figs/colab-badge.svg\" alt=\"Open In Colab\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ai72An86lqSS"
   },
   "outputs": [],
   "source": [
    "# do this step below to get lightning, lightning bolts, etc.\n",
    "# !pip install lightning-bolts torchvision torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdonR3h-PEn"
   },
   "source": [
    "# Classification with PyTorch (Lightning)\n",
    "\n",
    "*AY 128/256 (UC Berkeley, 2018–2024)*\n",
    "\n",
    "Here we'll work with pytorch lightning; its a wrapper around pytorch that provides a little more structure to the code.  There's no need to use it for this lab, though you're welcome to do so! \n",
    "\n",
    "Let's now explore classification, on images. Let's introduce the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist#labels) dataset: 70k small (28$\\times$28) images of 10 different types of clothing.\n",
    "\n",
    "<img src=\"https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJxs6AOJ-PEp"
   },
   "source": [
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "let's define the labels in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5B6atmbVWAa7"
   },
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o566vXl-PEq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from IPython.external import mathjax\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, ModelSummary\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# use a GPU or MPS (Mac) if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(\"pytorch version:\", torch.__version__)\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCwBscaPiTtv"
   },
   "source": [
    "Let's get the training and testing data \n",
    "\n",
    "**note that I've included the .csv files locally so you don't have to download these as written below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgaGA_kBiTtv"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -q \"https://github.com/fpleoni/fashion_mnist/blob/master/fashion-mnist_train.csv?raw=true\" --output-document=fashion-mnist_train.csv\n",
    "wget -q \"https://github.com/fpleoni/fashion_mnist/blob/master/fashion-mnist_test.csv?raw=true\" --output-document=fashion-mnist_test.csv\n",
    "gzip fashion-mnist_train.csv\n",
    "gzip fashion-mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uG5avg7hiTtx"
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"./fashion-mnist_train.csv.gz\")\n",
    "test_csv = pd.read_csv(\"./fashion-mnist_test.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOL83YFQiTty"
   },
   "outputs": [],
   "source": [
    "%%writefile fashion_dataset.py\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, transform = None):\n",
    "        \"\"\"Method to initilaize variables.\"\"\" \n",
    "        self.fashion_MNIST = list(data.values)\n",
    "        self.transform = transform\n",
    "        \n",
    "        label = []\n",
    "        image = []\n",
    "        \n",
    "        for i in self.fashion_MNIST:\n",
    "             # first column is of labels.\n",
    "            label.append(i[0])\n",
    "            image.append(i[1:])\n",
    "        self.labels = np.asarray(label)\n",
    "        # Dimension of Images = 28 * 28 * 1. where height = width = 28 and color_channels = 1.\n",
    "        self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels[index]\n",
    "        image = self.images[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_dataset import FashionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOL83YFQiTty"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Transform data into Tensor that has a range from 0 to 1\n",
    "train_set = FashionDataset(train_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2, persistent_workers=True)\n",
    "test_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmNypYL0-PEr"
   },
   "outputs": [],
   "source": [
    "type(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb-24iwm-PEr"
   },
   "outputs": [],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJhfigpH-PEu"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image, label = train_set[0]\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "output_label(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RybvUvM_-PEv"
   },
   "source": [
    "To learn a model to predict the class of a given image, we could treat this 28$\\times$28 image as 1d input, like a stellar spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zq5ew8nh-PEv"
   },
   "outputs": [],
   "source": [
    "ind=20\n",
    "_ = plt.plot(train_set[ind][0].numpy().reshape(-1))\n",
    "plt.ylabel(\"normalized flux\")\n",
    "plt.xlabel(\"1D pixel index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Wyfn1k4-PEw"
   },
   "source": [
    "But this *clearly* jumbles the inherent spatial structure and local correlations found in natural images. Using just Dense layers in a NN we'd effectively be asking the network to learn these correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkv4gLbc-PEx"
   },
   "source": [
    "## Convolutional Neural Nets (ConvNets or CNNs)\n",
    "\n",
    "NNs built for images (or more generally, inputs with spatial structure).\n",
    "\n",
    "### Key Ideas: \n",
    "  - layers see only parts of each image (effectively all other weights are zero).\n",
    "  - some layers do simple operations on previous layers to reduce dimensionality (e.g., take the largest value in a a 3x3 range)\n",
    "  - \"Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.\"\n",
    " \n",
    "<img src=\"http://cs231n.github.io/assets/cnn/cnn.jpeg\">\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/depthcol.jpeg\">\n",
    "\n",
    "\"An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input - see discussion of depth columns in text below. \"\n",
    "\n",
    "cf. http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "<img src=\"figs/f2.png\">\n",
    "Source: http://www.nature.com/nature/journal/v521/n7553/fig_tab/nature14539_F2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDMy90B6-PEy"
   },
   "source": [
    "### Filter banks\n",
    "\n",
    "  http://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml0bs_8U-PEy"
   },
   "source": [
    "### Pooling\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/pool.jpeg\" width=\"40%\">\n",
    "<img src=\"http://cs231n.github.io/assets/cnn/maxpool.jpeg\" width=\"40%\">\n",
    "Source: http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/convnets.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHd9NXJ9-PEz"
   },
   "source": [
    "We want our output predictions to look like a \"probability\" of belonging to one of the 10 classes. And, importantly, we'd like to make sure that the probability over all classes sums to unity. One way to do this is to scale the outputs of the last layer using a [`softmax`](https://en.wikipedia.org/wiki/Softmax_function):\n",
    "\n",
    "$$\n",
    "{\\rm softmax}(\\vec s) = \\frac{e^{s_i}}{\\sum_i e^{s_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTPwpm9t-PE0"
   },
   "source": [
    "So if the (unnormalized) prediction from am NN for an image is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5qSvdX5-PE0"
   },
   "outputs": [],
   "source": [
    "s = np.random.normal(size=(10,))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb9kUxsZ-PE0"
   },
   "source": [
    "Then the softmax scaling gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9uCUoUa-PE1"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "print(softmax(s))\n",
    "np.testing.assert_almost_equal(softmax(s).sum(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QmXQ7xERB09"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax as sp_softmax\n",
    "sp_softmax(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZIiiTry-PE1"
   },
   "source": [
    "We'll use the \"categorical cross-entropy\" loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS3Sopro-PE2"
   },
   "source": [
    "<img src=\"https://gombru.github.io/assets/cross_entropy_loss/softmax_CE_pipeline.png\">\n",
    "Source: https://gombru.github.io/2018/05/23/cross_entropy_loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" \n",
    "    Converts a class vector (integers) to a binary class matrix.\n",
    "\n",
    "    Args:\n",
    "        y: A tensor of class indices (integers).\n",
    "        num_classes: The total number of classes.\n",
    "\n",
    "    Returns:\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    return torch.eye(num_classes)[y]\n",
    "\n",
    "nb_classes = 10\n",
    "train_label = torch.tensor([train_set[i][1] for i in range(10)])\n",
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(train_label, nb_classes)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P7vAxRG-PE2"
   },
   "outputs": [],
   "source": [
    "# perfect match ... use a small ϵ to avoid taking log(0) since lim x log x -> 0 as x->0\n",
    "print(\"loss with a perfect match:\", -(y_train[0].double() @ np.log(y_train[0].double() + 1e-16)).numpy())\n",
    "print(\"loss with a predicted match:\", -(y_train[0].double() @ np.log(softmax(s) + 1e-16)).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRlyeM7J-PE3"
   },
   "source": [
    "## Building a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHGC36yoiTt8"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83LYcCd2iTt8"
   },
   "outputs": [],
   "source": [
    "class mycnn(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # set this to an example input size to the see a summary\n",
    "        # see https://pytorch-lightning.readthedocs.io/en/latest/common/debugging.html\n",
    "        self._example_input_array = torch.randn((1, 1, 28, 28))\n",
    "\n",
    "        # define the layers here\n",
    "        # Conv2d(in_channels, out_channels, kernel_size)\n",
    "        # see https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            \n",
    "            # see https://github.com/sksq96/pytorch-summary/issues/55#issuecomment-471844028\n",
    "            # to understand why pytorch and keras differ here\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1=torch.nn.Linear(1152, 32)\n",
    "        self.fc2=torch.nn.Linear(32, nb_classes)\n",
    "\n",
    "        # Negative Log Liklihood Loss\n",
    "        # see https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x=torch.relu(self.fc1(x))\n",
    "        x=F.log_softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.75,\n",
    "            patience=2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_accuracy\"}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss\n",
    "    \n",
    "    def _evaluate(self, batch, batch_idx, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=nb_classes)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_accuracy', acc, prog_bar=True)\n",
    "        if stage == \"val\":\n",
    "            self.val_losses.append(loss.item())  # Track validation loss\n",
    "            self.val_accuracies.append(acc.item())  # Track validation accuracy\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._evaluate(batch, batch_idx, 'val')[0]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the model we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model=mycnn()\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=nn_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukdCwzPRiTt9"
   },
   "outputs": [],
   "source": [
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "filename = f'datalab_nn_pytorch_{run_time_string}'\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_accuracy',\n",
    "   min_delta=0.03,\n",
    "   patience=3,\n",
    "   verbose=True,\n",
    "   mode='max'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    dirpath='nn_results',\n",
    "    filename=filename,\n",
    "    verbose=True,\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "logger = [CSVLogger(\"nn_results1\", name=filename), TensorBoardLogger(\"nn_results\", name=filename)]\n",
    "\n",
    "# reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "if device == \"gpu\":\n",
    "    myTrainer=pl.Trainer(callbacks=[early_stop_callback, checkpoint_callback], logger=logger,\n",
    "                                    gpus=-1, accelerator='auto', auto_select_gpus=True, max_epochs=5)\n",
    "else:\n",
    "    myTrainer=pl.Trainer(callbacks=[early_stop_callback, checkpoint_callback], logger=logger,\n",
    "                                    accelerator='auto', max_epochs=5)\n",
    "model=mycnn()\n",
    "myTrainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(savgol_filter(model.train_losses, 30, 3), label=\"train\",alpha=0.6)\n",
    "plt.plot(savgol_filter(model.val_losses, 30, 3), label=\"val\",alpha=0.6)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Epoch\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch of data from the test set\n",
    "data = next(iter(test_loader))\n",
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = !ls -t nn_results/*\n",
    "cpt = cpt[0]\n",
    "print(cpt)\n",
    "\n",
    "# Load up the model class\n",
    "model = mycnn.load_from_checkpoint(cpt).to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Example input data, replace with your actual data\n",
    "example_input = data[0].to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    predictions = model(example_input)\n",
    "\n",
    "# If you want the class with the highest probability\n",
    "predicted_class = torch.argmax(predictions, dim=1)\n",
    "\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1] # these are the real labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test = torch.Tensor(test_set.images).reshape(-1, test_set.images.shape[3], \n",
    "                                                 test_set.images.shape[1], \n",
    "                                                 test_set.images.shape[2])\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    predictions = model(all_test.to(device))\n",
    "predicted_class = torch.argmax(predictions, dim=1)\n",
    "\n",
    "conf_mat = confusion_matrix(test_set.labels, predicted_class.cpu())\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(conf_mat_normalized)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {0: \"T-shirt/top\",\n",
    "          1: \"Trouser\",\n",
    "          2: \"Pullover\",\n",
    "          3: \"Dress\",\n",
    "          4: \"Coat\",\n",
    "          5: \"Sandal\",\n",
    "          6: \"Shirt\",\n",
    "          7: \"Sneaker\",\n",
    "          8: \"Bag\",\n",
    "          9: \"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_wrong = []\n",
    "for i, (pred, actual) in enumerate(zip(predicted_class.cpu(),test_set.labels)):\n",
    "    if pred != actual:\n",
    "        ind_wrong.append((i, pred.item(), actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_wrong[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 10\n",
    "plt.imshow(all_test[ind_wrong[ind][0]][0,:,:], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"pred={lookup[ind_wrong[ind][1]]} true={lookup[ind_wrong[ind][2]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXWdI2td-PE5"
   },
   "source": [
    "### Aside: Dropout \n",
    "\n",
    "You'll notice above that the `accuracy` is much higher than the `val_accuracy`. That is, we overfit on the training data. One way to help protect against this is to introduce `Dropout`\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*iWQzxhVlvadk6VAJjsgXgg.png\">\n",
    "\n",
    "Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”, JMLR 2014\n",
    "\n",
    "```python\n",
    "        x = self.layer3(x)\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.Dropout(p=0.25)(x) # drop 25% \n",
    "        x=torch.relu(self.fc1(x))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeIpKHxB-PE5"
   },
   "source": [
    "### Aside: Visualization of the layers\n",
    "\n",
    "From François Chollet (“DEEP LEARNING with Python”):\n",
    "\n",
    "Intermediate activations are “useful for understanding how successive convnet layers transform their input, and for getting a first idea of the meaning of individual convnet filters.”\n",
    "\n",
    "“The representations learned by convnets are highly amenable to visualization, in large part because they’re representations of visual concepts. Visualizing intermediate activations consists of displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input (the output of a layer is often called its activation, the output of the activation function). This gives a view into how an input is decomposed into the different filters learned by the network. Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image.”\n",
    "\n",
    "Following from https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sD8Ajblof7bp"
   },
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Ensure the model is on the same device as the data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "data, _ = next(iter(train_set))\n",
    "data = data.unsqueeze(0).to(device)  # Move data to the same device\n",
    "\n",
    "for layer_name, layer in [('layer1', model.layer1), \n",
    "                          ('layer2', model.layer2)]:\n",
    "    layer.register_forward_hook(get_activation(layer_name))\n",
    "    output = model(data)\n",
    "\n",
    "    layer_activation = activation[layer_name].squeeze().cpu()  # Move activation to CPU for visualization\n",
    "\n",
    "    images_per_row = 15\n",
    "    n_features = layer_activation.shape[0]   # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] # The feature map has shape (n_features, size, size).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[col * images_per_row + row,\n",
    "                                              :, :]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255)\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                          row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcFKFine-PE6"
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "Another way to avoid overfitting, aside from `Dropout`, is to increase the number of exmaples used to to train the model.  Data augmentation is a generic term for methods used to expand the effect training set size by generating more data from the original training set. In images, this is pretty natural: scale changes, rotations, flips, etc. should still give us the same label. This method has the benefit of usually increasing test-time accuracy.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*C8hNiOqur4OJyEZmC7OnzQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyrkit-P-PE7"
   },
   "source": [
    "In Pytorch see https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html and https://pytorch.org/vision/stable/transforms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbxdGLblmeKe"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.RandomAffine(degrees=15, shear=0.1),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_set = FashionDataset(train_csv, transform=train_transforms)\n",
    "test_set = FashionDataset(test_csv, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2)\n",
    "test_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNQ6SWYwmjqX"
   },
   "outputs": [],
   "source": [
    "image, label = next(iter(train_set))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "output_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ftp5zXGFpi29"
   },
   "outputs": [],
   "source": [
    "class mycnn_dropout(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # set this to an example input size to the see a summary\n",
    "        # see https://pytorch-lightning.readthedocs.io/en/latest/common/debugging.html\n",
    "        self._example_input_array = torch.randn((1, 1, 28, 28))\n",
    "\n",
    "        # define the layers here\n",
    "        # Conv2d(in_channels, out_channels, kernel_size)\n",
    "        # see https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            \n",
    "            # see https://github.com/sksq96/pytorch-summary/issues/55#issuecomment-471844028\n",
    "            # to understand why pytorch and keras differ here\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(p=0.1)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc1=torch.nn.Linear(1152, 32)\n",
    "        self.fc2=torch.nn.Linear(32, 10)\n",
    "    \n",
    "        self.loss = nn.NLLLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # add dropout \n",
    "        x = nn.Dropout(p=0.2)(x)\n",
    "\n",
    "        x=torch.relu(self.fc1(x))\n",
    "        x=F.log_softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.75,\n",
    "            patience=2,\n",
    "            min_lr=1e-6,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_accuracy\"}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _evaluate(self, batch, batch_idx, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=nb_classes)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f'{stage}_loss', loss, prog_bar=True)\n",
    "            self.log(f'{stage}_accuracy', acc, prog_bar=True)\n",
    "\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._evaluate(batch, batch_idx, 'val')[0]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTooZRBQqHZq"
   },
   "outputs": [],
   "source": [
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "filename = f'datalab_nn_pytorch_dropout_{run_time_string}'\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_accuracy',\n",
    "   min_delta=0.001,\n",
    "   patience=3,\n",
    "   verbose=True,\n",
    "   mode='max'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    dirpath='nn_results',\n",
    "    filename=filename,\n",
    "    verbose=True,\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "logger = [CSVLogger(\"nn_results1\", name=filename), TensorBoardLogger(\"nn_results\", name=filename)]\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "if device == \"gpu\":\n",
    "    myTrainer=pl.Trainer(callbacks=[early_stop_callback, checkpoint_callback], logger=logger,\n",
    "                     gpus=-1, accelerator='dp', auto_select_gpus=True, max_epochs=5)\n",
    "else:\n",
    "    myTrainer=pl.Trainer(callbacks=[early_stop_callback, checkpoint_callback], logger=logger,\n",
    "                         max_epochs=5)\n",
    "    \n",
    "model_dropout=mycnn_dropout()\n",
    "myTrainer.fit(model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBJPKYc-Zfcy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RDMy90B6-PEy",
    "IXWdI2td-PE5"
   ],
   "name": "02_CNNs-pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "astrods",
   "language": "python",
   "name": "astrods"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
