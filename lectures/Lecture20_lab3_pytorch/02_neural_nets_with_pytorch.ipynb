{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ucb-datalab/course_materials_fall2024/blob/main/lectures/Lecture20_lab3_cnn/02_neural_nets_with_pytorch.ipynb\" target=\"_blank\" rel=\"nofollow\">\n",
    "    <img src=\"figs/colab-badge.svg\" alt=\"Open In Colab\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Frameworks\n",
    "\n",
    "AY 250 (UC Berkeley; 2012-2024)\n",
    "\n",
    "<div class=\"alert alert-info\">sklearn is not built for deep/complex networks such as required in covnets (as we'll see later on). We must go to specialized software (and potentially specialized hardware)</div>\n",
    "\n",
    "Almost all frameworks written in low-level C++/C with Python (or other scripting bindings)\n",
    "\n",
    "### Low-level frameworks\n",
    "\n",
    "   - Tensorflow (Google) Nov 2015. See https://www.tensorflow.org/api_docs/python/tf\n",
    "   - pytorch (Python). https://pytorch.org/docs/stable/index.html\n",
    "   - Theano\n",
    "   - Caffe (Berkeley)\n",
    "   - Torch (Lua)\n",
    "   - CNTK (Microsoft)\n",
    "   - Chainer\n",
    "   - PaddlePaddle (Baidu) Aug 2016\n",
    "   \n",
    "### High-level frameworks (Python)\n",
    "\n",
    "   - Keras (atop Tensorflow, Theano) - https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "   - Skorch - scikit-learn compatible neural network library that wraps PyTorch (https://github.com/skorch-dev/skorch)\n",
    "   - FastAI: https://docs.fast.ai/\n",
    "   - PyTorch Lightning (https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "   \n",
    "<img src=\"figs/frameworks.png\" width=\"75%\">\n",
    "Source: https://paperswithcode.com/trends\n",
    "\n",
    "\n",
    "see also: https://github.com/mbadry1/Top-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png\">\n",
    "\n",
    "Source: http://www.asimovinstitute.org/neural-network-zoo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example (from Josh's student work): \n",
    "\n",
    "<img src=\"https://github.com/profjsb/deepCR/raw/master/imgs/network.png\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/profjsb/deepCR/master/imgs/postage-sm.jpg\">\n",
    "\n",
    "\"deepCR: Deep Learning Based Cosmic Ray Removal for Astronomical Images\"\n",
    "https://github.com/profjsb/deepCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "   - syntax closely resembles Python, making it easy for developers familiar with the language to transition to deep learning.\n",
    "   - Dynamic Computational Graphs:\n",
    "Unlike some frameworks that require you to define the entire computation graph upfront (static graphs), PyTorch builds the graph on the fly as you execute your code. This makes debugging easier and allows for more flexibility, especially in research settings where rapid prototyping is crucial.\n",
    "   - PyTorch leverages the power of GPUs to accelerate computations, making it suitable for training large and complex models efficiently.                                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the California housing data as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "cal_data = datasets.fetch_california_housing()\n",
    "X = cal_data['data']   # 8 features \n",
    "Y = cal_data['target'] # response (median house price)\n",
    "\n",
    "half = math.floor(len(Y)/2)\n",
    "train_X = X[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X[half:]\n",
    "test_Y = Y[half:]\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_features = train_X.shape[1]\n",
    "print(f'number of input features = {num_input_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Print torch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple neural network in PyTorch, as an MLP with\n",
    "a few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClf(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network classifier using PyTorch.\n",
    "\n",
    "    This class defines a neural network with three hidden layers and one output layer.\n",
    "    It uses ReLU (Rectified Linear Unit) activation function for the hidden layers.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    layer1 : nn.Linear\n",
    "        The first linear layer which takes input features and maps them to 32 neurons.\n",
    "    layer2 : nn.Linear\n",
    "        The second linear layer which takes 32 neurons from the first layer and maps them to another 32 neurons.\n",
    "    layer3 : nn.Linear\n",
    "        The third linear layer which takes 32 neurons from the second layer and maps them to 10 neurons.\n",
    "    output_layer : nn.Linear\n",
    "        The output layer which takes 10 neurons from the third layer and maps them to a single output.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    forward(x):\n",
    "        Defines the forward pass of the neural network. It takes an input tensor `x` and passes it through\n",
    "        the layers with ReLU activation functions in between, and finally through the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features):\n",
    "        super(NNClf, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, 10)\n",
    "        self.output_layer = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNClf(num_input_features)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_Y_tensor = torch.tensor(train_Y, dtype=torch.float32).view(-1, 1)  # reshape to a 2D tensor\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float32)\n",
    "test_Y_tensor = torch.tensor(test_Y, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_Y_tensor.size())\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & DataLoaders\n",
    "\n",
    "\"Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\"\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorDataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset DataLoader\n",
    "train_dataset = TensorDataset(train_X_tensor, train_Y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "\n",
    "# The criterion is defined as Mean Squared Error (MSE) Loss, which is a common loss function \n",
    "# used for regression tasks. It measures the average squared difference between the \n",
    "# estimated values and the actual value.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# The optimizer is defined as Adam (Adaptive Moment Estimation) optimizer, which is an \n",
    "# algorithm for first-order gradient-based optimization of stochastic objective functions. \n",
    "# It is configured to optimize the parameters of the model with a learning rate of 0.001.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 50  # Number of epochs to train the model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    # Iterate over batches of data from the training loader\n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        optimizer.zero_grad()  # Clear the gradients of all optimized tensors\n",
    "        outputs = model(batch_X)  # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        loss = criterion(outputs, batch_Y)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "    print(f\"{loss.item():0.3f}\", sep=\" \", end=\" \", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # set outselves in evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "mse = np.mean((predictions - test_Y_tensor.numpy()) ** 2)\n",
    "print(\"MSE\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how well did we do?\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"NN Regression Residuals - MSE = %.3f\" % mse)\n",
    "plt.scatter(test_Y_tensor.numpy(), predictions, alpha=0.4, s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0.2, 5], [0.2, 5], c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complete Example\n",
    "\n",
    "We want to train and make some decisions of when to stop based on `validation` data. Ultimately, we'd like to see how well our model would do on truly new data (`test`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage, valid_percentage, test_percentage = (0.90, 0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rnd = np.random.RandomState(42)\n",
    "\n",
    "# make an array with the indices of all the rows in the dataset\n",
    "ind_arr = np.arange(X.shape[0])\n",
    "rnd.shuffle(ind_arr)\n",
    "\n",
    "train_ind, tmp = train_test_split(ind_arr, train_size=train_percentage, random_state=rnd)\n",
    "valid_ind, test_ind = \\\n",
    "      train_test_split(tmp, train_size=valid_percentage/(valid_percentage + test_percentage), \n",
    "                               random_state=rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that we're getting all the indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.array(sorted(list(train_ind) + list(valid_ind) + list(test_ind)))  == \\\n",
    "              sorted(ind_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind.shape, test_ind.shape, valid_ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scale the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "\n",
    "train_X = X[train_ind]\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test, validation data\n",
    "test_X = scaler.transform(X[test_ind])\n",
    "valid_X = scaler.transform(X[valid_ind])\n",
    "\n",
    "train_y = Y[train_ind] ; test_y = Y[test_ind] ; valid_y = Y[valid_ind]\n",
    "\n",
    "assert train_y.shape[0] == train_X.shape[0]\n",
    "assert test_y.shape[0] == test_X.shape[0]\n",
    "assert valid_y.shape[0] == valid_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_input_features):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_input_features, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, 5)\n",
    "        self.layer4 = nn.Linear(5, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(5, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SimpleNN(num_input_features)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for results\n",
    "try:\n",
    "    os.mkdir('nn_results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "model_path = f'nn_results/ay256_nn_{run_time_string}.pt'\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    print(f'Found GPU at: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('GPU device not found')\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(torch.tensor(train_X, dtype=torch.float32), torch.tensor(train_y, dtype=torch.float32))\n",
    "valid_dataset = TensorDataset(torch.tensor(valid_X, dtype=torch.float32), torch.tensor(valid_y, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        batch_y = batch_y.view(-1, 1)  # Reshape target to match model output\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_X, batch_y in valid_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            batch_y = batch_y.view(-1, 1)  # Reshape target to match model output\n",
    "\n",
    "            outputs = model(batch_X)\n",
    "            val_loss += criterion(outputs, batch_y).item()\n",
    "        val_loss /= len(valid_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_y = model(torch.tensor(test_X, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    pred_y = pred_y[:, 0]  # Flatten the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(test_y, pred_y)\n",
    "print(\"MSE\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(test_y, pred_y, alpha=0.4, s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0.2, 5], [0.2, 5], c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do compared to the validation and training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in hyperparameter optimization, you could try out Weights & Biases for Keras (https://wandb.ai/site/articles/intro-to-keras-with-weights-biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Training on a GPU (on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    print('GPU device not found')\n",
    "    has_gpu = False\n",
    "else:\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    has_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
