{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to statistical inference\n",
    "\n",
    "\n",
    "- For scientists, the role of inference is to draw quantitative conclusions from noisy data.\n",
    "\n",
    "- Historically, approaches to inference can be divided into two camps termed ‘frequentist’ and ‘Bayesian’. The frequentist interpretation of probability is expressed in terms of repeated trials, while Bayesian interpret probability as a degree of belief.\n",
    "\n",
    "- Statiscian Larry Wasserman has [written about the distinction](https://normaldeviate.wordpress.com/2012/11/17/what-is-bayesianfrequentist-inference/) emphasizing that the two camps are defined by what outcome they hope to achieve rather than defined by what methods they use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The methodological implications of this distinction are profound and subtle. The notion of hypothesis testing is drawn from frequentist statistics, where propositions are evaluated by the possibility of their being false (e.g., p-values). On the other hand, concepts such as likelihood and evidence arise from within Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While the distinction is not considered a major rift within statistics today, the application of inference within scientific fields is often surprisingly one-sided: e.g., within cosmology Bayesian inference is standard, in particle physics frequentist statistics are the norm. Astronomy as a whole is more mixed, what about other fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to statistical inference in science\n",
    "\n",
    "• Ultimately, the difference between frequentist and Bayesian statistics is not of the highest practical importance for scientists. The kinds of questions that matter to scientists are:\n",
    "\n",
    "- “I have these data with some error bars (that, between the two of us, I do not trust). I want to publish in Nature. What do I do in between?”\n",
    " \n",
    "- Or, more specifically: “How do I fit a model to these data, or decide which of two models is better?”\n",
    "  \n",
    "- Or, even more specifically: “How do I take this `numpy` array and find maximum likelihood parameters with an associated covariance matrix and/or joint probability distributions (while leaving enough time to finish my lab report)?”\n",
    "\n",
    "\n",
    "Approaches to statistical inference with Python (ie., actually deal with data). That is packages that interface with `numpy`, return \"optimized\" numbers (that are probably\n",
    "arguments in a function call), as well as some description of the probability distribution from which they are drawn (an array? a function to draw samples?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian parameter inference: formalism\n",
    "\n",
    "When embarking upon an experiment, we almost always have some prior expectation about the outcome. Bayesian inference is the process by which this expectation (or \"belief\") is updated to account for new data we obtain.\n",
    "\n",
    "\n",
    " Information about parameters is expressed in terms of probability distributions:\n",
    " <img src=\"data/bayes.png\">\n",
    " \n",
    "In Bayesian statistics, we perform inferences with posterior probability distributions on parameters of interest, θ, given some data X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief History of Bayesian Stats\n",
    "\n",
    "1. Thomas Bayes (1702–1761), a minister & amateur mathematician, proved a case of Bayes’ Theorem in a 1763 paper.\n",
    "2. Pierre-Simon Laplace (1749–1827) introduced a general version of the theorem and applied it to several fields, including celestial mechanics & medicine. When insufficient knowledge was available to specify an informed prior, Laplace used uniform priors, according to his “principle of insufficient reason”\n",
    "3. Fell out of favor in the early 20th century, where Frequentist Statistics of Fisher, Neyman, and Pearson dominated the field\n",
    "4. Around 1950, statisticians began to advocate Bayesian methods to overcome the limitations of frequentist approach: L.J. Savage, Bruno de Finetti, Dennis Lindley\n",
    "5. Bayesian Statistics did not become popular until the 1980’s and 90’s:\n",
    "the Bayesian approach requires evaluation of complex, multi-dimensional integrals\n",
    " Faster and cheaper computing along with efficient sampling algorithms led to the revitalization of the field and wide-spread acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective versus Subjective Bayes\n",
    "\n",
    "- An essential ingredient to obtaining the posterior,  $p(\\theta | {\\rm data})$, is the prior distribution,  $p(\\theta)$, symbolizing our belief in $\\theta$ before collecting or observing any data\n",
    "- The prior can have a large impact on the inferences and opens one up to charges of non-objectivity\n",
    "- However, by the same argument, the choice of Likelihood function (probability model for the data, given the model parameters) used by both Bayesians and Frequentists is also subjective\n",
    "- There is a lot of work attempting to minimize the effect of the prior on resulting inference.  These are non-informative or reference priors\n",
    "- “Subjective” Bayesians believe in the complete subjectivity of the interpretation of probability and believe that informative priors should always be used, if available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in Bayesian Analysis:\n",
    "1. Specify likelihood and prior (before looking at the data!)\n",
    "2. Compute the posterior distribution for the parameter(s) of interest given the particular X that we observed.\n",
    "  - In cases where direct derivation of the posterior is impossible, we instead draw samples from the posterior\n",
    "3. Check that the model fits well (posterior predictive checks)\n",
    "4. Perform statistical inferences (parameter estimation, predictions on new data, model comparison)\n",
    "\n",
    "\n",
    "<u>Reading and References</u>\n",
    "\n",
    "- [\"Bayesian Methods for Hackers\"](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers): An introduction to Bayesian methods + probabilistic programming with a computation/understanding-first, mathematics-second point of view. All in pure Python.\n",
    "\n",
    "- [\"Bayesian Data Analysis 3 (BDA3)\"](http://www.stat.columbia.edu/~gelman/book/): The \"bible\" of Bayesian analysis by Andrew Gelman, et al. \n",
    "\n",
    "- Salvatier, J, Wiecki TV, and Fonnesbeck C. (2016) Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 https://doi.org/10.7717/peerj-cs.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presidental Election Polls \n",
    "\n",
    "In a recent poll by Data for Progress (Sept 13, 2024), there where $n=1,283$ respondents of likely voters (https://www.dataforprogress.org/blog/2024/9/13/post-debate-harris-leads-by-4-as-voters-see-her-as-more-honest-composed-and-intelligent-than-trump)\n",
    "\n",
    "**We want to estimate the true proportion, $\\theta$, of voting Americans that favor one candidate over another.**\n",
    "\n",
    "What is a sensible likelihood $p(X | \\theta$)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: the **[Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution)**\n",
    "\n",
    "$$\n",
    "p(X | \\theta) = \\binom{n}{a} \\theta^a (1 - \\theta)^{n - a}\n",
    "$$\n",
    "where $n$ is the total number in the poll and $a$ is the number that approve. \n",
    "$$\n",
    "\\binom{n}{a}  = \\frac{n!}{a! (n - a)!}\n",
    " $$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a sensible prior $p(\\theta)$? We could choose a flat prior (equal probability of $\\theta$ between 0 and 1) but that's probably not reasonable. Note that if we choose a prior of the form $\\propto \\theta^r (1 - \\theta)^{s}$, then our **posterior** will have the same form. A common prior distribution is the [**Beta distribution**](https://en.wikipedia.org/wiki/Beta_distribution).\n",
    "\n",
    "$$\n",
    "{\\rm Beta}(\\alpha, \\beta) \\propto \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "alpha_betas = [(0.5,0.5), (1,1), (3,3), (4,1), (10,10), (100,100)]\n",
    "\n",
    "x_theta = np.linspace(0, 1, 101)\n",
    "plt.figure(figsize=(10,6))\n",
    "for alpha, beta in alpha_betas:\n",
    "    p_theta = stats.beta(alpha, beta).pdf(x_theta)\n",
    "    plt.plot(x_theta, p_theta, linewidth=3.,label=f\"{alpha}, {beta}\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim([0, max(p_theta)])\n",
    "plt.xlabel(\"$\\\\theta$\")\n",
    "plt.ylabel(\"$p(\\\\theta)$\")\n",
    "plt.title(\"Beta distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(\\theta | X )  \\propto  \\theta^a (1 - \\theta)^{n - a} \\times \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\propto   \\theta^{\\alpha + a - 1} (1 - \\theta)^{n - a + \\beta - 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\propto {\\rm Beta(\\alpha + a, \\beta + n - a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the poll noted above, there were $a$=667 who support Harris for president $n$=1283 (we split the 4% undecided to each candidate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 667\n",
    "n = 1283\n",
    "\n",
    "alpha_betas = [(0.5,0.5), (1,1), (3,3), (4,1), (10,10), (100,100)]\n",
    "\n",
    "x_theta = np.linspace(0, 1, 101)\n",
    "plt.figure(figsize=(10,6))\n",
    "for alpha, beta in alpha_betas:\n",
    "    p_theta = stats.beta(alpha + a, beta + n - a).pdf(x_theta)\n",
    "    plt.plot(x_theta, p_theta, linewidth=3.,label=f\"{alpha}, {beta}\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim([0, max(p_theta)])\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "plt.ylabel(r\"$p(\\theta)$\")\n",
    "plt.title(\"Posterior distribution: Harris for President\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our choice of priors had very little effect. This is exactly what we expect when we have a lot of data. What if we only polled 13 people (say n=13, a=7)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 7\n",
    "n = 13\n",
    "\n",
    "alpha_betas = [(0.5,0.5), (1,1), (3,3), (4,1), (10,10)]\n",
    "\n",
    "x_theta = np.linspace(0, 1, 101)\n",
    "plt.figure(figsize=(10,6))\n",
    "for alpha, beta in alpha_betas:\n",
    "    p_theta = stats.beta(alpha + a, beta + n - a).pdf(x_theta)\n",
    "    plt.plot(x_theta, p_theta, linewidth=3.,label=f\"{alpha}, {beta}\")\n",
    "    \n",
    "plt.legend()\n",
    "plt.ylim([0, max(p_theta)])\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "plt.ylabel(r\"$p(\\theta)$\")\n",
    "plt.title(\"Posterior distribution Small Sample: Harris for President\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our prior belief holds a lot of weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Congugate Priors\n",
    "\n",
    "- with a congugate prior, the posterior takes a closed form. In particular, the posterior has the same functional distribution as the prior.\n",
    "\n",
    "- computation of the posterior with congugate priors are trivial. The posterior parameters are easy to solve.\n",
    "\n",
    "- As we collect even more data (under the same liklihood), the posterior can be used as the next prior and it will be congugate. This allows for easy updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With most practical problems in Bayesian inference, conjugacy does not exist.  We need to use computationally intensive sampling procedures to characterize the posterior distribution!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
