{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e1b0ec",
   "metadata": {},
   "source": [
    "# Fitting Tutorial\n",
    "\n",
    "Below is a basic exercise in fitting a quadratic polynomial to some noisy data, done using many different fitting packages to demonstrate their basic usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa0b50-2e7d-42f2-87f8-30b739b785b8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\chi^2 = \\sum_i^n \\frac{(x_{i, \\rm true} - x_{i, \\rm model})^2}{\\sigma_{x_i}^2}\n",
    "$$\n",
    "\n",
    "Reduced $\\chi^2$:\n",
    "$$\n",
    "\\chi^2_r = \\chi^2/(n - {\\rm len} \\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacc950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:36:59.223033Z",
     "start_time": "2024-03-21T23:36:58.876654Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652830e",
   "metadata": {},
   "source": [
    "## The True Model (and Data Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff484d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:36:59.227665Z",
     "start_time": "2024-03-21T23:36:59.224653Z"
    }
   },
   "outputs": [],
   "source": [
    "def model(x, a, b, c):\n",
    "    '''The model we are fitting.'''\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "x = np.linspace(-5, 5, 256)\n",
    "SIG_N = 1\n",
    "prms_true = (1.1, 2.2, 3.3)\n",
    "y_meas = model(x, *prms_true) + SIG_N * np.random.normal(size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b94b51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:36:59.242491Z",
     "start_time": "2024-03-21T23:36:59.228647Z"
    }
   },
   "outputs": [],
   "source": [
    "def report(prms):\n",
    "    '''Generate a pretty report and plot for a set of parameters.'''\n",
    "    if type(prms) == dict:\n",
    "        prms = (prms['a'], prms['b'], prms['c'])\n",
    "    if type(prms[0]) != float:\n",
    "        prms = tuple(float(p) for p in prms)\n",
    "    y = model(x, *prms)\n",
    "    chisq = np.sum(np.abs(y_meas - y)**2 / SIG_N**2)\n",
    "    chisq_r = chisq / (y_meas.size - len(prms))\n",
    "    print(f'A:{prms[0]:7.3}, B:{prms[1]:7.3}, C:{prms[2]:7.3}')\n",
    "    print(f'Chi-sq: {chisq:7.3}, Reduced Chi-sq: {chisq_r:7.3}')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, y_meas, '.', label='measured')\n",
    "    plt.plot(x, y, label='model')\n",
    "    plt.plot(x, y_meas - y, label='residual')\n",
    "    plt.grid()\n",
    "    _ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb433f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:36:59.386346Z",
     "start_time": "2024-03-21T23:36:59.258212Z"
    }
   },
   "outputs": [],
   "source": [
    "report(prms_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5db90-6cef-4732-b391-19642f21add7",
   "metadata": {},
   "source": [
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea4119",
   "metadata": {},
   "source": [
    "## numpy.linalg.lstsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d47e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:36:59.524745Z",
     "start_time": "2024-03-21T23:36:59.388804Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.linalg.lstsq\n",
    "# generic matrix-based linear chi-sq optimizer\n",
    "A = np.array([x**2, x, np.ones_like(x)]).T  # design matrix\n",
    "prms_opt, res2, rank, s = np.linalg.lstsq(A, y_meas, rcond=None)\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf43bbe",
   "metadata": {},
   "source": [
    "## numpy.polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d6fe49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:36:59.647460Z",
     "start_time": "2024-03-21T23:36:59.529222Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.polyfit\n",
    "# special case of matrix-based linear chi-sq optimizer for polynomials\n",
    "prms_opt = np.polyfit(x, y_meas, deg=2)\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb8727",
   "metadata": {},
   "source": [
    "##  scipy.optimize.fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d65b3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:36:59.938990Z",
     "start_time": "2024-03-21T23:36:59.649082Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "def my_chisq(prms, x, y_meas, sig_n=1):\n",
    "    return np.sum(np.abs(model(x, *prms) - y_meas)**2 / sig_n**2)\n",
    "\n",
    "# scipy.optimize.fmin\n",
    "# very generic function optimization, guesses gradient from serial function evaluations\n",
    "init_guess = np.array([0.1, 0.1, 0.1])\n",
    "prms_opt, chisq_min, niter, ncalls, flags = fmin(my_chisq, init_guess, (x, y_meas, SIG_N), full_output=True)\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643847b5",
   "metadata": {},
   "source": [
    "## scipy.optimize.curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e00c81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:00.063945Z",
     "start_time": "2024-03-21T23:36:59.941021Z"
    }
   },
   "outputs": [],
   "source": [
    "# scipy.optimize.curve_fit\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# generic function optimization via least-squares, guesses gradient from serial function evaluations\n",
    "init_guess = np.array([0.1, 0.1, 0.1])\n",
    "prms_opt, cov, info, msg, flag = curve_fit(model, x, y_meas, init_guess, SIG_N * np.ones_like(y_meas), full_output=True)\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81dca7",
   "metadata": {},
   "source": [
    "## linsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f958c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:00.219335Z",
     "start_time": "2024-03-21T23:37:00.075295Z"
    }
   },
   "outputs": [],
   "source": [
    "# linsolve interface to np.linalg.lstsq\n",
    "import linsolve\n",
    "\n",
    "data = {f'a*{xi**2}+b*{xi}+c': yi for xi, yi in zip(x, y_meas)}\n",
    "ls = linsolve.LinearSolver(data)\n",
    "prms_opt = ls.solve()\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593bd17",
   "metadata": {},
   "source": [
    "## jax (and optax)\n",
    "\n",
    "Note that (for now) <tt>optax</tt> does not support complex numbers (it fails silently), so build complex numbers inside jax from real-valued optax parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07f749-792c-4eb3-9aa7-b0ee873df389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:00.687028Z",
     "start_time": "2024-03-21T23:37:00.220712Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "def sim(prms, args):\n",
    "    x, _, _ = args\n",
    "    return prms['a'] * x**2 + prms['b'] * x + prms['c']\n",
    "\n",
    "@jax.jit\n",
    "def calc_loss(prms, args):\n",
    "    _, y_meas, sig_n = args\n",
    "    loss = jnp.sum(jnp.abs(y_meas - sim(prms, args))**2 / sig_n**2)\n",
    "    return loss\n",
    "\n",
    "def fit(optimizer, prms, args, maxiter=150, print_every=100, target_loss=1):\n",
    "    opt_state = optimizer.init(prms)\n",
    "    \n",
    "    def step(opt_state, prms, args):\n",
    "        loss, grads = jax.value_and_grad(calc_loss)(prms, args)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, prms)\n",
    "        prms = optax.apply_updates(prms, updates)\n",
    "        return prms, opt_state, loss, grads\n",
    "    \n",
    "    for i in range(maxiter):\n",
    "        prms, opt_state, loss, grads = step(opt_state, prms, args)\n",
    "        if i % print_every == 0 or i == maxiter-1 or loss < target_loss:\n",
    "            print(f'step {i:04d}, loss: {loss:9.7f}')\n",
    "        if loss < target_loss:\n",
    "            break\n",
    "    return prms, loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cc6aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:05.068694Z",
     "start_time": "2024-03-21T23:37:00.713793Z"
    }
   },
   "outputs": [],
   "source": [
    "prms = {'a': jnp.asarray([1.0]), 'b': jnp.asarray([1.0]), 'c': jnp.asarray([1.0])}\n",
    "optimizer = optax.lamb(learning_rate=3e-3)\n",
    "prms_opt, chisq_min, _ = fit(optimizer, prms, (x, y_meas, SIG_N), maxiter=500, target_loss=y_meas.size-len(prms)-4)\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299a940-9958-4dd6-a628-e6158be3f414",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo (MCMC) methods\n",
    "\n",
    "The approaches below all use MCMC methods to converge toward the Bayesian posterior. MCMC methods tend to be slower to converge than gradient-based methods, but they have the advantage of exploring around the maximum likelihood solution to explicitly examine the posterior probability (e.g. final error bars) of fit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357c6c6-695c-4901-a2eb-88f2d0091513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import tqdm\n",
    "\n",
    "prm_order = ['a', 'b', 'c', 'logsig']\n",
    "nprms = len(prm_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088e244-251a-4871-83de-34db5c5cc4db",
   "metadata": {},
   "source": [
    "## pymc\n",
    "\n",
    "<tt>pymc</tt> is a relatively batteries-included MCMC package. Running it is pretty simple, but beware that the way a chain is set up relies implicitly on class instantiation, making its evaluation a little hard for a reader to follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2083d0-9011-43fc-addb-48acc50b90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc\n",
    "\n",
    "def run_mcmc(x, y_meas):\n",
    "    with pymc.Model() as mdl:\n",
    "        a = pymc.Uniform('a', -5, 5)\n",
    "        b = pymc.Uniform('b', -5, 5)\n",
    "        c = pymc.Uniform('c', -5, 5)\n",
    "        logsig = pymc.Uniform('logsig', -1.0, 1.0)\n",
    "        y_err = 10**logsig\n",
    "        yhat = model(x, a, b, c)\n",
    "        likelihood = pymc.Normal(\"y\", mu=yhat, sigma=y_err, observed=y_meas)\n",
    "        return pymc.sample(2_000, tune=200)\n",
    "\n",
    "chain = run_mcmc(x, y_meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa7bcc-b2e2-4e44-bd62-df089543867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=nprms, sharex=True, figsize=(8,8))\n",
    "for i, k in enumerate(prm_order):\n",
    "    axes[i].plot(chain.posterior[k].T)\n",
    "    axes[i].set_ylabel(k)\n",
    "axes[nprms-1].set_xlabel('Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0921b8-c870-4122-ba6a-6e092cae634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = corner.corner(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3048c9f3-dd1a-4692-9cf5-43402203bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prms_opt = {k: np.mean(chain.posterior[k]) for k in prm_order}\n",
    "sig_n = 10**prms_opt.pop('logsig')\n",
    "print(f'Inferred noise sigma: {sig_n:7.3}')\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104a69b",
   "metadata": {},
   "source": [
    "## emcee\n",
    "\n",
    "<tt>emcee</tt> requires more explicit programming, which makes it harder to use but easier to read/control. Note the custom classes created here to handle priors more cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715eaf0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:08.157701Z",
     "start_time": "2024-03-21T23:37:05.070397Z"
    }
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "class Uniform:\n",
    "    '''Model a flat (uniform) prior of a variable.'''\n",
    "    def __init__(self, name, lower, upper):\n",
    "        self.name = name\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "    def start_val(self):\n",
    "        return np.random.uniform(self.lower, self.upper)\n",
    "    def log_prior(self, val):\n",
    "        if self.lower < val < self.upper:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return -np.inf\n",
    "        \n",
    "class Normal:\n",
    "    '''Model a normal (gaussian distributed) prior of a variable.'''\n",
    "    def __init__(self, name, mean, sigma):\n",
    "        self.name = name\n",
    "        self.mean = mean\n",
    "        self.sigma = sigma\n",
    "    def start_val(self):\n",
    "        return self.mean + self.sigma * np.random.normal()\n",
    "    def log_prior(self, val):\n",
    "        return -0.5*(np.log(2 * np.pi * self.sigma**2) + np.abs(val - self.mean)**2 / self.sigma**2 )\n",
    "        \n",
    "        \n",
    "prms = {\n",
    "    'a': Uniform('a', lower=-5, upper=5),\n",
    "    'b': Uniform('b', lower=-5, upper=5),\n",
    "    'c': Uniform('c', lower=-5, upper=5),\n",
    "    'logsig': Uniform('logsig', lower=-1.0, upper=1.0),\n",
    "}\n",
    "        \n",
    "def log_prior(**kwargs):\n",
    "    return sum([prms[k].log_prior(v) for k, v in kwargs.items()])\n",
    "\n",
    "def log_likelihood(y, yhat, sig_noise):\n",
    "    L = -0.5 * (np.log(2 * np.pi * sig_noise**2) + np.abs(y - yhat)**2 / sig_noise**2 )\n",
    "    return np.sum(L)\n",
    "\n",
    "def log_probability(prms, x, y_meas):\n",
    "    a, b, c, logsig = prms\n",
    "    prms = dict(zip(prm_order, prms))\n",
    "    lp = log_prior(**prms)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    yhat = model(x, a, b, c)\n",
    "    return lp + log_likelihood(y_meas, yhat, 10**logsig)\n",
    "\n",
    "nwalkers = 12\n",
    "sol0 = np.array([[prms[k].start_val() for _ in range(nwalkers)] for k in prm_order]).T\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, nprms, log_probability, args=(x, y_meas))\n",
    "sampler.run_mcmc(sol0, 2500, progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6ceee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:08.702561Z",
     "start_time": "2024-03-21T23:37:08.159091Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note in emcee that we have to explicitly discard the early \"burn-in\" samples of a chain.\n",
    "trace = sampler.get_chain(discard=500)\n",
    "trace = dict(zip(prm_order, trace.T))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nprms, sharex=True, figsize=(8,8))\n",
    "for i, k in enumerate(prm_order):\n",
    "    axes[i].plot(trace[k].T)\n",
    "    axes[i].set_ylabel(k)\n",
    "axes[nprms-1].set_xlabel('Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673d065",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:09.700966Z",
     "start_time": "2024-03-21T23:37:08.703859Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = corner.corner(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0ba1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T23:37:09.828031Z",
     "start_time": "2024-03-21T23:37:09.702483Z"
    }
   },
   "outputs": [],
   "source": [
    "prms_opt = {k: np.mean(trace[k]) for k in prm_order}\n",
    "sig_n = 10**prms_opt.pop('logsig')\n",
    "print(f'Inferred noise sigma: {sig_n:7.3}')\n",
    "report(prms_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57997535-1b7f-4e62-a981-bff0cb54d173",
   "metadata": {},
   "source": [
    "## manual MCMC\n",
    "\n",
    "A final (not recommended) option is to code your own MCMC walker. Some of the code for this was already built above for <tt>emcee</tt>, so we just need to add the Metropolis-Hastings stepper to make it work. However, it is very finicky about step sizes, burn-in, and correlation between steps, making it substantially inferior to most modern MCMC packages. It does, however, illustrate the basics of how MCMC works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae637e4-6298-42b7-a828-823293452923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metropolis_hastings_step(current, step_size):\n",
    "    proposed = current + np.random.normal(scale=step_size, size=current.size)\n",
    "    acceptance_ratio = np.exp(log_probability(proposed, x, y_meas) - log_probability(current, x, y_meas))\n",
    "    if np.random.uniform() < acceptance_ratio: \n",
    "        current = proposed\n",
    "    return current\n",
    "\n",
    "def metropolis_hastings(n_steps, n_burnin, step_size, prms):\n",
    "    current = np.array([prms[k].start_val() for k in prm_order])\n",
    "    chain = []\n",
    "    for _ in range(n_burnin - 1):\n",
    "        current = _metropolis_hastings_step(current, step_size)\n",
    "    for _ in tqdm.tqdm(range(n_steps)):\n",
    "        current = _metropolis_hastings_step(current, step_size)\n",
    "        chain.append(current)\n",
    "    chain = dict(zip(prm_order, np.array(chain).T))\n",
    "    return chain\n",
    "\n",
    "chain = metropolis_hastings(12000, 6200, 0.02, prms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f300baa-0c14-471a-8f8d-d79d13baca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=nprms, sharex=True, figsize=(8,8))\n",
    "for i, k in enumerate(prm_order):\n",
    "    axes[i].plot(chain[k].T)\n",
    "    axes[i].set_ylabel(k)\n",
    "axes[nprms-1].set_xlabel('Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb0f10-8a3e-49b0-8cd8-b7b848652d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = corner.corner(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e73c68-f146-4b7e-bde3-07145a9a3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prms_opt = {k: np.mean(chain[k]) for k in prm_order}\n",
    "sig_n = 10**prms_opt.pop('logsig')\n",
    "print(f'Inferred noise sigma: {sig_n:7.3}')\n",
    "report(prms_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
