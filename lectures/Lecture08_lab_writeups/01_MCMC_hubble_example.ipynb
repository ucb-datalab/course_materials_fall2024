{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MCMC to fit real data\n",
    "\n",
    "### AY 128/256 (UC Berkeley, 2021-2024)\n",
    "\n",
    "* Let's look at a famous dataset\n",
    "<img src=\"hubble_diagram.jpg\" width=\"90%\">\n",
    "* Hubble (1929) finds that more distant galaxies have larger positive velocities than closer galaxies\n",
    "* Ends up being a big deal\n",
    "* What were some of our crisiticms of this plot, and by extension the result, from our first meeing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our challenge\n",
    "* Hubble found $H_0 \\sim 500$ km/s/Mpc, more or less fitting data with no error bars by eye.\n",
    "* Let's see how well this result stands up to more detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Wrangling\n",
    "* We need to get Hubble's data\n",
    "* Fortunately, it's in Table 1 of Hubble (1929)\n",
    "<img src=\"hubble_table.jpeg\" width=\"80%\">\n",
    "\n",
    " - $m_s$ = photographic magnitude of brightest stars involved.\n",
    "\n",
    " - $r$ = distance in units of $10^6$ parsecs. The first two are Shapley’s values.\n",
    "\n",
    " - $v$ = measured velocities in km/s.    NGC 6822, 221, 224 and 5457 are recent determinations by Humason.\n",
    "\n",
    " - $m_t$ = Holetschek’s visual magnitude as corrected by Hopmann. The first three objects were not measured by Holetschek, and the values of mt represent estimates by the author based upon such data as are available.\n",
    "\n",
    " - $M_t$ = total visual absolute magnitude computed from m_t and $r$.\n",
    "\n",
    "* Since it's a short table, I turned this into a csv file by hand.  It contains galaxy name, distance, velocity, and absolute magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Visualization\n",
    "* Let's use Pandas to read in the data and try to re-create Hubble's plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "# read in the csv file; column names are in the file already\n",
    "df = pd.read_csv('hubble1929.tab1')\n",
    "\n",
    "import pylab as plt\n",
    "# plot distance and velocity\n",
    "plt.scatter(df['distance'], df['velocity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's make this plot look a bit nicer using Seaborn\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n",
    "\n",
    "cmap = plt.get_cmap('plasma')\n",
    "\n",
    "plt.scatter(df['distance'], df['velocity'], c=df['absMag'], s=150, edgecolor='0.7', cmap=cmap)\n",
    "plt.xlabel('Distance (Mpc)', size=24)\n",
    "plt.ylabel('Velocity (km/s)', size=24)\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"Absolute Magnitude\", rotation=270, labelpad=20, fontsize=18)\n",
    "\n",
    "# make colorbar brighest to faintest\n",
    "cb.ax.invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Write down a model for the data\n",
    "* $y_n = m~x_n + b$\n",
    " - Linear model to descibe velocity as a function of distance\n",
    " - 2 parameters to fit for: $m$ (slope) and $b$ (offset)\n",
    " - $y_n$: velocity of the nth galaxy; $x_n$: distance to the nth galaxy\n",
    " - We have no error bars (i.e., no $\\sigma_n$) -- is this a problem? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Error bars\n",
    "* Let's fit for the scatter in the data (or the size of the typical uncertainty)\n",
    "* Assume errors are normally distributed\n",
    "* error term: $lnsig = \\log(\\sigma)$\n",
    "* why use $\\log(\\sigma)$ instead of $\\sigma$ as a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Review Bayes's theorem\n",
    "* Bayes's theorem: $P(\\theta \\mid \\{y_n\\}) \\propto P(\\{y_n\\} \\mid \\theta) \\, P(\\theta)$\n",
    " - $\\theta = \\{m,b,\\log(\\sigma)\\}$\n",
    "* Assume we have $N$ independent data points (i.e., galaxies in this case).  The posterior for the entire dataset is the product of the posteriors for each data point.\n",
    "\n",
    " - $P(\\theta \\mid \\{y_n\\}) = \\prod_n^N P(\\theta \\mid y_n) \\propto\\ \\prod_n^N P( y_n \\mid \\theta)\\, P(\\theta)$ \n",
    " \n",
    "\n",
    "* Or in log-space:\n",
    " - $\\log P(\\theta \\mid \\{y_n\\}) \\propto \\sum_n^N \\log P(y_n \\mid \\theta) + \\log P(\\theta)$\n",
    " - why work in log-space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write down priors and log-likelihood\n",
    "\n",
    "* Uniform prior on $m$:\n",
    "\n",
    " $$\n",
    "P(m)=\n",
    "\\begin{cases}\n",
    "1 \\, \\rm{if} \\, 100 < m < 1000\\\\\n",
    "0 \\, {\\rm otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Uniform prior on $b$:\n",
    "\n",
    " $$\n",
    "P(b)=\n",
    "\\begin{cases}\n",
    "1 \\, \\rm{if} -500 < b < 500\\\\\n",
    "0 \\, {\\rm otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Uniform prior on $\\log(\\sigma)$:\n",
    "\n",
    " $$\n",
    "P(b)=\n",
    "\\begin{cases}\n",
    "1 \\, \\rm{if} -10 < \\log(\\sigma) < 10\\\\\n",
    "0 \\, {\\rm otherwise}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    " \n",
    "\n",
    "* Likelihood function for a normal distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\{y_n\\} \\mid \\{m, b, \\log(\\sigma)\\}) = \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{(y_n - mx_n - b)^2}{2 \\sigma_n^2}}\n",
    "\\end{equation}\n",
    "\n",
    "   - where $x_n$ is shorthand for the 'distance' variable\n",
    "\n",
    "\n",
    "* log-Likelihood function for a normal distribution:\n",
    "\n",
    " $ \\log P(\\{y_n\\} \\mid \\{m, b, \\log(\\sigma)\\} = -\\frac{1}{2} \\sum^{N}_{n=1} \\Big(\\frac{(y_n - mx_n - b)^2}{\\sigma_n^2} + \\log(2 \\pi \\sigma_n^2) \\Big)$\n",
    " \n",
    "   - how might this change if we had values for $\\sigma_n$ already?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of writing this model:\n",
    "* $m \\sim {\\rm Uniform}(100, 1000)$\n",
    "* $b \\sim {\\rm Uniform}(-500, 500)$\n",
    "* lnsig = $\\log(\\sigma) \\sim {\\rm Uniform}(-10,10)$\n",
    "* $y_n \\sim {\\rm Normal}(m x_n +b, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 5 & 6: Implement and run the model in python using PyMC, make some diagnostic plots\n",
    "\n",
    "We'll use the built-in Normal to create a likelihood. To define and use your own likelihood (eg. log-likelihood) you can do a Custom function in pymc. See https://www.pymc.io/projects/examples/en/latest/howto/blackbox_external_likelihood_numpy.html#introduction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "from arviz import plot_trace as traceplot\n",
    "\n",
    "# set up the model\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    # define priors\n",
    "    m = pm.Uniform(\"m\", lower=100, upper=1000)\n",
    "    b = pm.Uniform(\"b\", lower=-500, upper=500)\n",
    "    lnsig = pm.Uniform(\"lnsig\", lower=-10, upper=10)\n",
    "    \n",
    "    likelihood = pm.Normal(\"obs\", mu = m*df['distance'].values+b, \n",
    "              sigma=np.exp(lnsig), observed=df['velocity'].values)\n",
    "\n",
    "    # now set up the model to run\n",
    "    # default of PyMC is to use the no-turn sampler (NUTS)\n",
    "    \n",
    "    # pm.sample will run the sampler and store output in 'trace' \n",
    "    trace = pm.sample(draws=5000, tune=1000, chains=4, cores=4, discard_tuned_samples=True)\n",
    "    \n",
    "    # traceplot is a routine for plotting the 'traces' from the samples\n",
    "    _ = traceplot(trace, var_names=[\"m\", \"b\", \"lnsig\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm.summary provides some useful summary and convergance statistics\n",
    "az.summary(trace, var_names=[\"m\", \"b\", \"lnsig\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`corner` is a nice routine for visualizing the marginalized posterior distributions\n",
    "\n",
    "  - 1d plots (1d histograms) are called **marginalized distributions**\n",
    "  - 2d plots (2d histograms) are called **joint distributions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "# make the corner plot and plot results from Hubble's paper as 'truth'\n",
    "# overplot percentiles: 16, 50, 84 on 1d historgrams\n",
    "_ = corner.corner(trace, truths=[500, 0, None], quantiles=[.16, .50, .84], show_titles=True,\n",
    "    title_kwargs={\"fontsize\": 12})\n",
    "\n",
    "# how big is the scatter/typical error in the data?\n",
    "# let's use the median of logsig for this\n",
    "print(f\"The typical error in velocity is ~{int(np.median(np.exp(trace.posterior['lnsig'])))} km/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Posterior Predictive check\n",
    "* the technical term that means to plot your model on top of the data and do a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this plot look a bit nicer using Seaborn\n",
    "\n",
    "#import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n",
    "plt.scatter(df['distance'], df['velocity'], c=df['absMag'], s=150, edgecolor='0.7', cmap=cmap)\n",
    "plt.xlabel('Distance (Mpc)', size=24)\n",
    "plt.ylabel('Velocity (km/s)', size=24)\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"Absolute Magnitude\", rotation=270, labelpad=20, fontsize=18)\n",
    "cb.ax.invert_yaxis()\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# this code loops over the trace array and pulls 500 random sets of m,b, and logsig\n",
    "# it then generates 500 posterior models (i.e., lines in velocity and distance space) and overplots them on the data\n",
    "\n",
    "\n",
    "m_model = trace.posterior['m'].values.flatten()\n",
    "b_model = trace.posterior['b'].values.flatten()\n",
    "dist = np.asarray(df['distance'])\n",
    "\n",
    "\n",
    "for i in np.random.randint(0, len(m_model)-1, 500):\n",
    "    plt.plot(dist, m_model[i]*dist + b_model[i], color=\"k\", lw=0.1, alpha=0.3, zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
