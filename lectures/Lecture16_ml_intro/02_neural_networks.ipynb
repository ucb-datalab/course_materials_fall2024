{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install --upgrade pydot-ng\n",
    "pip install python-resize-image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks & Multi-Layer Perceptrons\n",
    "\n",
    "DataLab (AY128/256: UC Berkeley 2016-2024; J. Bloom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is machine learning with neural networks?\n",
    "\n",
    "   - A set of methodologies for doing machine learning\n",
    "   - Collection/composition of simple mathematical functions whose parameterization is *learned* by passing over the data\n",
    "   - \"Deep Learning\": modern version of \"artificial neural networks\"\n",
    "   \n",
    "### Why do people like it?\n",
    "   - It's \"inspired\" by how the brain is thought to work, so it *feels* like a natural approach. \n",
    "   - It works. Amazingly well. In a growing number of use cases.\n",
    "   - It's composeable, so it's \"easy\" to understand each piece.\n",
    "   - Featurizes + learns on \"raw\" data.\n",
    "   - Timely: It's tractable with the data/problems we have and the compute power we have access to.\n",
    "   - New shiny object with codebases getting commoditized (read: easier and easier to use...and free)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do people dislike it?\n",
    "\n",
    "   - Decades of hype\n",
    "   - It's considered a black box in a lot of ways\n",
    "   - It can take a seasoned expert to get it right\n",
    "   - It's expensive to run/learn a model\n",
    "   - Not natively adapted to heterogenous data and certain types of learning\n",
    "   - Not the approach of choice for small/medium data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://fastml.com/images/ai/new_navy_device_learns_by_doing.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Neuron\" (\"Perceptron\")\n",
    "<img src=\"https://www.evernote.com/l/AUVUbm0I38pMWbUhfC0VUZv7qxxguDOy64QB/image.png\">\n",
    "Source: http://www.wsdm-conference.org/2016/slides/WSDM2016-Jeff-Dean.pdf\n",
    "\n",
    "<img width=\"50%\" src=\"http://www.webpages.ttu.edu/dleverin/neural_network/fig_3p6_neurons_NEURON4.jpg\">\n",
    "Source: http://www.webpages.ttu.edu/dleverin/neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is to *learn* the weights $w_i$ given the data. This is done as such:\n",
    "\n",
    "  1. Initialization:\n",
    "      - Set the transfer (e.g. sum) & activation (sigmoid) functions you want to use.\n",
    "      - randomly assign the weights (with some probability distribution)\n",
    "  2. For each instance $i$, run your input $\\vec x_i$ through the network with current weights to get the current output.\n",
    "  3. Determine $\\Delta$ how far off the current output is from the true output/labels.\n",
    "  4. Update the weights by taking the gradient of the activation at $\\vec x_i$ and multiplying by $\\Delta$.\n",
    "  5. Repeat steps 2--5 until you hit a stopping criteria.\n",
    "  \n",
    "This process is an optimization and is called **\"Back Propogation\"** and, if your activation function is differentiable, it's basically a form of gradient descent and reduces to doing simple linear algebra to find the optimimal weights given the data.  It was first presented by Rumelhart, Hinton, Williams ([Nature, 1986](http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Network Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "\n",
    "# output label\n",
    "Y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(wx):\n",
    "    \"\"\"\n",
    "    how to aggregate the weighted inputs\n",
    "\n",
    "    here we'll just do a sum over the weights times x\n",
    "    $\\Sigma_i w_i x_i$\n",
    "    \"\"\"\n",
    "    return np.sum(wx, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different activation functions:\n",
    "\n",
    "<img src=\"https://imiloainf.files.wordpress.com/2013/11/activation_funcs1.png\" width=\"50%\">\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(twx, func=\"ReLU\", derivative=False):\n",
    "    \"\"\"\n",
    "    activation: how to treat the sum of the weighted input (twx)    \n",
    "    \"\"\"\n",
    "    if func == \"ReLU\":\n",
    "        if derivative:\n",
    "            return np.array([0 if x <= 0 else 1 for x in twx])\n",
    "        return np.array([max(0, x) for x in twx])\n",
    "    \n",
    "    elif func == \"sigmoid\":\n",
    "        if derivative:\n",
    "            return np.array([x*(1-x) for x in twx])\n",
    "        return np.array([1/(1+np.exp(-x)) for x in twx])\n",
    "    \n",
    "    elif func == \"tanh\":\n",
    "        if derivative:\n",
    "            np.array([1 - (np.tanh(x))**2 for x in twx])\n",
    "        return np.array([np.tanh(x) for x in twx])\n",
    "\n",
    "    else:\n",
    "        print(f\"func {func} not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights_initial = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_error = {\"tanh\": [], \"sigmoid\": []}\n",
    "\n",
    "for func in [\"tanh\", \"sigmoid\"]:\n",
    "\n",
    "    weights = weights_initial.copy()\n",
    "\n",
    "    for _ in range(10000):\n",
    "\n",
    "        # forward propagation\n",
    "        layer0 = X\n",
    "        sum_of_weighted_X = transfer(layer0*weights.T)\n",
    "        layer1 = activation(sum_of_weighted_X, func=func)\n",
    "\n",
    "        # how much did we miss?\n",
    "        layer1_error = Y.T - layer1\n",
    "\n",
    "        rms_error[func].append(np.sqrt((layer1_error**2).sum()))\n",
    "        # multiply how much we missed by the\n",
    "        # slope of the activation at the values in layer1\n",
    "        layer1_delta = layer1_error * activation(layer1, derivative=True)\n",
    "\n",
    "        weights += np.dot(layer1_delta, layer0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.plot(rms_error[\"sigmoid\"],label=\"sigmoid\")\n",
    "plt.plot(rms_error[\"tanh\"],label=\"tanh\")\n",
    "\n",
    "plt.xscale(\"symlog\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"RMS Error\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.legend()\n",
    "plt.title(\"Single Layer NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we updated the weights at each pass by using all the instances (this is called \"batch learning\"). There are speed ups (but generally noisier learning) by randomly choosing a subset of the data at each iteration (\"stochastic learning\").\n",
    "\n",
    "See [LeCun, Bottou, Orr, & Muller 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Networks\n",
    "\n",
    "Multilayer networks are not really any different. They have more weights to learn but they may also represent more complex models. Backpropogation optimization still works, this time by using the chain rule. That is, optimization is multi-step but it's  local to individual layers (this makes the problem tractable).\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" width=\"50%\">\n",
    "\n",
    "The above network is said to have a hidden layer, which is neither an input nor an output layer.\n",
    "\n",
    "In sklearn there are a few solver for backpropogation optimization:\n",
    "  - ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "  - ‘sgd’ refers to stochastic gradient descent.\n",
    "  - ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs',activation=\"tanh\",\n",
    "                    hidden_layer_sizes=(5,2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "\n",
    "# output label\n",
    "Y = np.array([0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run draw_nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_MLP_model(fig.gca(),clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iterations:\", clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(np.array([1,0,1]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(np.array([1,1,0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "\n",
    "# output label\n",
    "Y = [[\"sad\", \"mouse\"], [\"happy\", \"dog\"], [\"sad\", \"mouse\"], [\"happy\", \"oski\"]]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y1 = mlb.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs',activation=\"tanh\",\n",
    "                    hidden_layer_sizes=(15,10,6), random_state=1)\n",
    "clf.fit(X, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_MLP_model(fig.gca(),clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newx = np.array([0,0,1]).reshape(1,3)\n",
    "clf.predict(newx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "cal_data = datasets.fetch_california_housing()\n",
    "X = cal_data['data']   # 8 features \n",
    "Y = cal_data['target'] # response (median house price)\n",
    "half = math.floor(len(Y)/2)\n",
    "train_X = X[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X[half:]\n",
    "test_Y = Y[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(activation=\"tanh\",alpha=0.1,solver='sgd',\n",
    "                   nesterovs_momentum=False, learning_rate_init=0.2,\n",
    "                   hidden_layer_sizes=(20,20,20,5), random_state=1, max_iter = 400)\n",
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(test_Y,clf.predict(test_X),alpha=0.3,s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0,5], [0,5], \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[multi-layer neural nets in the Browser]( http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.65948&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
